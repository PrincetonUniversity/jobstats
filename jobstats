#!/usr/bin/python3
import argparse
import csv
import datetime
import os
import subprocess
import sys
import time
import requests
import json
import base64
import gzip
from blessed import Terminal
import syslog

# for convenience
DEVNULL = open(os.devnull, 'w')
# it's what we need to get unix times
os.environ['SLURM_TIME_FORMAT']="%s"

# prometheus server to query
PROM_SERVER="http://vigilant2:8480"

# class that gets and holds per job prometheus statistics
class JobStats:
    # threshold values for red versus black notes
    gpu_utilization_red   =  15 # percentage
    gpu_utilization_black =  25 # percentage
    cpu_utilization_red   =  65 # percentage
    cpu_utilization_black =  80 # percentage
    min_runtime_seconds   = 300 # seconds
    min_memory_usage      =  70 # percentage
    cascade_max_mem       = 190 # GB
    physics_max_mem       = 380 # GB

    # initialize basic job stats, can be called either with those stats
    # provided and if not it will fetch them
    def __init__(self,
                 jobid=None,
                 jobidraw=None,
                 start=None,
                 end=None,
                 gpus=None,
                 cluster=None,
                 debug=False,
                 debug_syslog=False,
                 force_recalc=False,
                 simple=False,
                 color=("","","")):
        self.cluster = cluster
        self.debug = debug
        self.debug_syslog = debug_syslog
        self.force_recalc = force_recalc
        self.simple = simple
        self.sp_node = {}
        self.txt_bold   = color[0]
        self.txt_red    = color[1]
        self.txt_normal = color[2]
        if self.debug_syslog:
            syslog.openlog('jobstat[%s]' % jobid)
        if jobidraw == None:
            self.jobid = jobid
            if not self.__get_job_info():
                if self.state == "PENDING":
                    self.error("Failed to get details for job %s since it is a PENDING job." % jobid)
                else:
                    self.error("Failed to get details for job %s." % jobid)
        else:
            if jobid == None:
                jobid = jobidraw
            self.jobid = jobid
            self.jobidraw = jobidraw
            self.start = start
            self.end = end
            self.gpus = gpus
            self.data = None
            self.timelimitraw = None
        self.diff = self.end - self.start
        # for tiger data is collected as tiger but slurm cluster name is tiger2
        if self.cluster == "tiger2":
            self.cluster = "tiger"
        if self.cluster == "mcmillan3":
            self.cluster = "mcmillan"
        self.debug_print("jobid=%s, jobidraw=%s, start=%s, end=%s, gpus=%s, diff=%s, cluster=%s, data=%s, timelimitraw=%s" % 
            (self.jobid,self.jobidraw,self.start,self.end,self.gpus,self.diff,self.cluster,self.data,self.timelimitraw))
        if self.data != None and self.data.startswith('JS1:') and len(self.data)>10:
            try:
                t = json.loads(gzip.decompress(base64.b64decode(self.data[4:])))
                self.sp_node = t["nodes"]
            except Exception as e:
                print("ERROR: %s" %e)
        if not self.sp_node:
            # call prometheus to get detailed statistics, if long enough
            if self.diff > 59:
                self.get_job_stats()

    def nodes(self):
        return self.sp_node

    def jobid(self):
        return self.jobidraw

    def diff(self):
        return self.diff

    def gpus(self):
        return self.gpus

    # report an error on stderr and fail
    def error(self, msg):
        if __name__ == "__main__":
            sys.stderr.write("%s\n" % msg)
            if self.debug_syslog:
                syslog.syslog(msg)
            sys.exit(1)
        else:
            raise Exception(msg)

    def debug_print(self, msg):
        if self.debug:
            print('DEBUG: %s' % msg)
        if self.debug_syslog:
            syslog.syslog(msg)

    # Get basic info from sacct and set instance variables
    def __get_job_info(self):
        # jobname must be last field next line to handle "|" chars later on
        cmd = ["sacct", "-P", "-X", "-o",
               "jobidraw,start,end,cluster,reqtres,admincomment,user,account,state,nnodes,ncpus,reqmem,qos,partition,timelimitraw,jobname",
               "-j", self.jobid]
        if self.cluster:
            cmd += ["-M", self.cluster]
        self.start    = None
        self.end      = None
        self.jobidraw = None
        try:
            for i in csv.DictReader(subprocess.check_output(cmd,stderr=DEVNULL).decode("utf-8").split('\n'), delimiter='|'):
                self.jobidraw     = i.get('JobIDRaw',None)
                self.start        = i.get('Start',None)
                self.end          = i.get('End',None)
                self.cluster      = i.get('Cluster',None)
                self.tres         = i.get('ReqTRES',None)
                if self.force_recalc:
                    self.data     = None
                else:
                    self.data     = i.get('AdminComment',None)
                self.user         = i.get('User',None)
                self.account      = i.get('Account',None)
                self.state        = i.get('State',None)
                self.timelimitraw = i.get('TimelimitRaw',None)
                self.nnodes       = i.get('NNodes',None)
                self.ncpus        = i.get('NCPUS',None)
                self.reqmem       = i.get('ReqMem',None)
                self.qos          = i.get('QOS',None)
                self.partition    = i.get('Partition',None)
                self.jobname      = i.get('JobName',None)  # if "|" in jobname then will be truncated
                self.debug_print('jobidraw=%s, start=%s, end=%s, cluster=%s, tres=%s, data=%s, user=%s, account=%s, state=%s, timelimit=%s, nodes=%s, ncpus=%s, reqmem=%s, qos=%s, partition=%s, jobname=%s' % (self.jobidraw, self.start, self.end, self.cluster, self.tres, self.data, self.user, self.account, self.state, self.timelimitraw, self.nnodes, self.ncpus, self.reqmem, self.qos, self.partition, self.jobname))
        except Exception as e:
            self.error("Failed to lookup jobid %s" % self.jobid)
 
        if self.jobidraw == None:
            if self.cluster:
                self.error(f"Failed to lookup jobid %s on {self.cluster.replace('tiger2', 'tiger')}. Make sure you specified the correct cluster." % self.jobid)
            else:
                self.error("Failed to lookup jobid %s." % self.jobid)

        self.gpus = 0
        if self.tres != None and 'gres/gpu=' in self.tres and 'gres/gpu=0,' not in self.tres:
            for part in self.tres.split(","):
                if "gres/gpu=" in part:
                    self.gpus = int(part.split("=")[-1])
 
        if self.timelimitraw.isnumeric():
            self.timelimitraw = int(self.timelimitraw)
        if "CANCEL" in self.state:
          self.state = "CANCELLED"
        if len(self.jobname) > 32:
            self.jobname = self.jobname[:32] + "..."

        # currently running jobs will have Unknown as time
        if self.end == 'Unknown':
            self.end = time.time()
        else:
            if self.end.isnumeric():
                self.end = int(self.end)
            else:
                return False
        if self.start.isnumeric():
            self.start = int(self.start)
            return True
        else:
            return False

    # extract info out of what was returned
    # sp = hash indexed by node
    # d  = data returned from prometheus
    # n  = what name to give this data
    #{'metric': {'__name__': 'cgroup_memory_total_bytes', 'cluster': 'stellar', 'instance': 'stellar-m02n30:9306', 'job': 'Stellar Nodes', 'jobid': '50783'}, 'values': [[1629592582, '536870912000']]}
    # or
    #{'metric': {'cluster': 'stellar', 'instance': 'stellar-m06n4:9306', 'job': 'Stellar Nodes', 'jobid': '50783'}, 'value': [1629592575, '190540828672']}
    def get_data_out(self, d, n):
        if 'data' in d:
            j = d['data']['result']
            for i in j:
                node=i['metric']['instance'].split(':')[0]
                minor = i['metric'].get('minor_number', None)
                if 'value' in i:
                    v=i['value'][1]
                if 'values' in i:
                    v=i['values'][0][0]
                # trim unneeded precision
                if '.' in v:
                    v = round(float(v), 1)
                else:
                    v = int(v)
                if node not in self.sp_node:
                    self.sp_node[node] = {}
                if minor != None:
                    if n not in self.sp_node[node]:
                        self.sp_node[node][n] = {}
                    self.sp_node[node][n][minor] = v
                else:
                    self.sp_node[node][n] = v

    def get_data(self, where, query):
        # run a query against prometheus
        def __run_query(q, start=None, end=None, time=None, step=60):
            params = { 'query': q, }
            if start:
                params['start'] = start
                params['end'] = end
                params['step'] = step
                qstr = 'query_range'
            else:
                qstr = 'query'
                if time:
                    params['time'] = time
            response = requests.get('{0}/api/v1/{1}'.format(PROM_SERVER, qstr), params)
            return response.json()
        
        expanded_query = query%(self.cluster, self.jobidraw, self.diff)
        self.debug_print("query=%s, time=%s" %(expanded_query,self.end))
        try:
            j = __run_query(expanded_query, time=self.end)
        except Exception as e:
            self.error("ERROR: Failed to query jobstats database, got error: %s:" % e)
        self.debug_print("query result=%s" % j)
        if j["status"] == 'success':
            self.get_data_out(j, where)
        elif j["status"] == 'error':
            self.error("ERROR: Failed to get run query %s with time %s, error: %s" % (expanded_query, self.end, j["error"]))
        else:
            self.error("ERROR: Unknown result when running query %s with time %s, full output: %s" %(expanded_query, self.end, j))

    def get_job_stats(self):
        # query CPU and Memory utilization data
        self.get_data('total_memory', "max_over_time(cgroup_memory_total_bytes{cluster='%s',jobid='%s',step='',task=''}[%ds])")
        self.get_data('used_memory', "max_over_time(cgroup_memory_rss_bytes{cluster='%s',jobid='%s',step='',task=''}[%ds])")
        self.get_data('total_time', "max_over_time(cgroup_cpu_total_seconds{cluster='%s',jobid='%s',step='',task=''}[%ds])")
        self.get_data('cpus', "max_over_time(cgroup_cpus{cluster='%s',jobid='%s',step='',task=''}[%ds])")

        # and now GPUs
        if self.gpus:
            self.get_data('gpu_total_memory', "max_over_time((nvidia_gpu_memory_total_bytes{cluster='%s'} and nvidia_gpu_jobId == %s)[%ds:])")
            self.get_data('gpu_used_memory', "max_over_time((nvidia_gpu_memory_used_bytes{cluster='%s'} and nvidia_gpu_jobId == %s)[%ds:])")
            self.get_data('gpu_utilization', "avg_over_time((nvidia_gpu_duty_cycle{cluster='%s'} and nvidia_gpu_jobId == %s)[%ds:])")

    def human_bytes(self, size, decimal_places=1):
        size=float(size)
        for unit in ['B','KB','MB','GB','TB']:
            if size < 1024.0:
                break
            size /= 1024.0
        return f"{size:.{decimal_places}f}{unit}"

    def human_seconds(self, seconds):
        hour = seconds // 3600
        if hour >= 24:
            days = "%d-" % (hour // 24)
            hour %= 24
            hour = days + ("%02d:" % hour)
        else:
            if hour > 0:
                hour = "%02d:" % hour
            else:
                hour = '00:'
        seconds = seconds % (24 * 3600)
        seconds %= 3600
        minutes = seconds // 60
        seconds %= 60
        return "%s%02d:%02d" % (hour, minutes, seconds)

    def human_datetime(self, x):
       return datetime.datetime.fromtimestamp(x).strftime("%a %b %-d, %Y at %-I:%M %p")

    def simple_output(self):
        gutter = "  "
        # cpu time utilization
        print(f"{gutter}CPU utilization per node (CPU time used/run time)") 
        for node, used, alloc, cores in self.cpu_util__node_used_alloc_cores:
            msg = ""
            if used == 0: msg = f" {self.txt_bold}{self.txt_red}<--- CPU node was not used.{self.txt_normal}"
            print(f"{gutter}    {node}: {self.human_seconds(used)}/{self.human_seconds(alloc)} (efficiency={100 * used / alloc:.1f}%){msg}")
        used, alloc, _ = self.cpu_util_total__used_alloc_cores
        if self.nnodes != "1":
            print(f"{gutter}Total used/runtime: {self.human_seconds(used)}/{self.human_seconds(alloc)}, efficiency={100 * used / alloc:.1f}%")
        # cpu memory usage
        print(f"\n{gutter}CPU memory usage per node - used/allocated")
        for node, used, alloc, cores in self.cpu_mem__node_used_alloc_cores:
            print(f"{gutter}    {node}: {self.human_bytes(used)}/{self.human_bytes(alloc)} ", end="")
            print(f"({self.human_bytes(used*1.0/cores)}/{self.human_bytes(alloc*1.0/cores)} per core of {cores})")
        total_used, total, total_cores = self.cpu_mem_total__used_alloc_cores
        if self.nnodes != "1":
            print(f"{gutter}Total used/allocated: {self.human_bytes(total_used)}/{self.human_bytes(total)} ", end="")
            print(f"({self.human_bytes(total_used*1.0/total_cores)}/{self.human_bytes(total*1.0/total_cores)} per core of {total_cores})")
        if self.gpus:
            # gpu utilization
            print(f"\n{gutter}GPU utilization per node")
            if self.partition == "mig":
                print(f"{gutter}    {node} (GPU): GPU utilization is unknown for MIG jobs")
            else:
                for node, util, gpu_index in self.gpu_util__node_util_index:
                    msg = ""
                    if util == 0: msg = f" {self.txt_bold}{self.txt_red}<--- GPU was not used.{self.txt_normal}"
                    print(f"{gutter}    {node} (GPU {gpu_index}): {util}%{msg}")
            # gpu memory usage
            print(f"\n{gutter}GPU memory usage per node - maximum used/total")
            for node, used, total, gpu_index in self.gpu_mem__node_used_total_index:
                print(f"{gutter}    {node} (GPU {gpu_index}): {self.human_bytes(used)}/{self.human_bytes(total)} ({100.0*used/total:.1f}%)")

    def job_notes(self):
        s = ""
        #### red notes (severe) ###
        zero_gpu = False
        if self.gpus and (self.diff > JobStats.min_runtime_seconds):
            num_unused_gpus = sum([util == 0 for _, util, _ in self.gpu_util__node_util_index])
            if num_unused_gpus:
                if self.gpus == 1:
                    s +=f"  {self.txt_bold}{self.txt_red}* This job did not use the GPU. This is a waste of expensive resources. You\n"
                    s += "    need to resolve this before running additional jobs. Is the code GPU-enabled?\n"
                    s += "    Please consult the documentation for the code. For additional info:\n"
                    s +=f"    https://researchcomputing.princeton.edu/support/knowledge-base/gpu-computing{self.txt_normal}\n\n"
                else:
                    s +=f"  {self.txt_bold}{self.txt_red}* This job did not use {num_unused_gpus} of the {self.gpus} allocated GPUs. This is a waste of expensive\n"
                    s += "    resources. You need to resolve this before running additional jobs. Is the\n"
                    s += "    code capable of using multiple GPUs? Please consult the documentation for\n"
                    s += "    the code. For additional info:\n"
                    s +=f"    https://researchcomputing.princeton.edu/support/knowledge-base/gpu-computing{self.txt_normal}\n\n"
                zero_gpu = True
        #import pdb; pdb.set_trace()
        total_used, total, total_cores = self.cpu_mem_total__used_alloc_cores
        if (not zero_gpu) and (self.gpus == 1) and (self.cluster == "della") and (self.partition == "gpu") and (self.diff > JobStats.min_runtime_seconds) and \
           (self.gpu_utilization < JobStats.gpu_utilization_red) and (int(self.ncpus) == 1) and (total_used / 1024**3 < 32):
            gpu_mem_used, gpu_mem_alloc = self.gpu_mem_total__used_alloc
            if gpu_mem_used / 1024**3 < 10:
                if "sys/dashboard/sys/jupyter" in self.jobname:
                    s +=f"  {self.txt_bold}{self.txt_red}* This job should probably use a MIG GPU instead of a full A100 GPU. MIG is\n"
                    s += "    ideal for jobs with a low GPU utilization that only require a single CPU-core\n"
                    s += "    and less than 32 GB of CPU memory and less than 10 GB of GPU memory. To use\n"
                    s += '    MIG with OnDemand Jupyter, choose "mig" as the "Custom partition" when\n'
                    s += "    creating the session. For more info:\n"
                    s +=f"    https://researchcomputing.princeton.edu/support/knowledge-base/jupyter#environments{self.txt_normal}\n\n"
                elif self.jobname == "interactive":
                    s +=f"  {self.txt_bold}{self.txt_red}* This job should probably use a MIG GPU instead of a full A100 GPU. MIG is ideal\n"
                    s += "    for jobs with a low GPU utilization that only require a single CPU-core and less\n"
                    s += "    than 32 GB of CPU memory and less than 10 GB of GPU memory. To use MIG with\n"
                    s += "    salloc:\n\n"
                    s += "      $ salloc --nodes=1 --ntasks=1 --time=60:00 --gres=gpu:1 --partition=mig\n\n"
                    s += "    For more info:\n"
                    s +=f"    https://researchcomputing.princeton.edu/systems/della#gpus{self.txt_normal}\n\n"
                else:
                    s +=f"  {self.txt_bold}{self.txt_red}* This job should probably use a MIG GPU instead of a full A100 GPU. MIG is\n"
                    s += "    ideal for jobs with a low GPU utilization that only require a single CPU-core\n"
                    s += "    and less than 32 GB of CPU memory and less than 10 GB of GPU memory. For\n"
                    s += "    future jobs, please add the following line to your Slurm script:\n\n"
                    s += "      #SBATCH --partition=mig\n\n"
                    s += "    For more info:\n"
                    s +=f"    https://researchcomputing.princeton.edu/systems/della#gpus{self.txt_normal}\n\n"
        zero_cpu = False
        if (self.diff > JobStats.min_runtime_seconds):
            num_unused_nodes = sum([used == 0 for _, used, _, _ in self.cpu_util__node_used_alloc_cores])
            if num_unused_nodes:
                if self.nnodes == "1":
                    s +=f"  {term.txt_bold}{term.txt_red}* This job did not use the CPU. This suggests that something is going wrong\n"
                    s += "    at the very beginning of the job. Please resolve this before submitting\n"
                    s +=f"    additional jobs.{term.txt_normal}\n\n"
                else:
                    s +=f"  {self.txt_bold}{self.txt_red}* This job did not use {num_unused_nodes} of the {self.nnodes} allocated CPU nodes. You need to resolve\n"
                    s +=  "    this before running additional jobs. Is the code capable of using multiple\n"
                    s +=f"    nodes? Please consult the documentation for the code.{self.txt_normal}\n\n"
                zero_cpu = True
        if (not zero_gpu) and self.gpus and (self.gpu_utilization < JobStats.gpu_utilization_red):
            s +=f"  {self.txt_bold}{self.txt_red}* The overall GPU utilization of this job is {round(self.gpu_utilization)}%. Please investigate the reason\n"
            s += "    for the low utilization. For more info:\n"
            s +=f"    https://researchcomputing.princeton.edu/support/knowledge-base/gpu-computing{self.txt_normal}\n\n"
        if (not zero_cpu) and (not self.gpus) and (self.cpu_efficiency < JobStats.cpu_utilization_red):
            s +=f"  {self.txt_bold}{self.txt_red}* The overall CPU utilization of this job is {self.cpu_efficiency}%. This value is low. Please\n"
            if int(self.ncpus) > 1:
                s += "    investigate the reason for the low efficiency. Consider performing a scaling\n"
                s += "    analysis:\n"
                s +=f"    https://researchcomputing.princeton.edu/support/knowledge-base/scaling-analysis{self.txt_normal}\n\n"
            else:
                s +=f"    investigate the reason for the low efficiency.{self.txt_normal}\n\n"
        if (self.state == "OUT_OF_MEMORY"):
            s +=f"  {self.txt_bold}{self.txt_red}* This job failed because it exceeded the amount of allocated CPU memory. For\n"
            s +=f"    solutions, see this page:\n"
            s +=f"    https://researchcomputing.princeton.edu/support/knowledge-base/memory{self.txt_normal}\n\n"
        if (self.state == "TIMEOUT"):
            s +=f"  {self.txt_bold}{self.txt_red}* This job failed because it exceeded the time limit. The solution is to\n"
            s +=f"    increase the value of the --time directive as explained on this page:\n"
            s +=f"    https://researchcomputing.princeton.edu/support/knowledge-base/slurm{self.txt_normal}\n\n"
        #import pdb; pdb.set_trace()
        gb_per_core = round(total / total_cores / 1024**3, 1) if total_cores != 0 else 0
        cpu_memory_utilization = round(100 * total_used / total) if total != 0 else 0
        tense = "is" if self.state == "RUNNING" else "was"
        if (self.cluster == "stellar") and ("pu" in self.qos or "pppl" in self.qos) and (gb_per_core > 9) and \
           (cpu_memory_utilization < JobStats.min_memory_usage) and (self.state != "OUT_OF_MEMORY"):
            s +=f'  {self.txt_bold}{self.txt_red}* This job requested {str(gb_per_core).replace(".0", "")} GB of memory per CPU-core which is more than the\n'
            s +=f"    default value of 8 GB. Given that the overall CPU memory usage {tense} only {cpu_memory_utilization}%,\n"
            s += "    please consider reducing your CPU memory allocation for future jobs. This\n"
            s += "    will reduce your queue times and make the resources available for other\n"
            s += "    users. For more info:\n"
            s +=f"    https://researchcomputing.princeton.edu/support/knowledge-base/memory{self.txt_normal}\n\n"
        if (self.cluster == "stellar") and ("cimes" in self.qos) and (gb_per_core > 5) and \
           (cpu_memory_utilization < JobStats.min_memory_usage) and (self.state != "OUT_OF_MEMORY"):
            s +=f'  {self.txt_bold}{self.txt_red}* This job requested {str(gb_per_core).replace(".0", "")} GB of memory per CPU-core which is more than the\n'
            s +=f"    default value of 4 GB. Given that the overall CPU memory usage {tense} only {cpu_memory_utilization}%,\n"
            s += "    please consider reducing your CPU memory allocation for future jobs. This\n"
            s += "    will reduce your queue times and make the resources available for other\n"
            s += "    users. For more info:\n"
            s +=f"    https://researchcomputing.princeton.edu/support/knowledge-base/memory{self.txt_normal}\n\n"
        if (self.cluster == "traverse") and (gb_per_core > 4) and \
           (cpu_memory_utilization < JobStats.min_memory_usage) and (self.state != "OUT_OF_MEMORY"):
            s +=f'  {self.txt_bold}{self.txt_red}* This job requested {str(gb_per_core).replace(".0", "")} GB of memory per CPU-core which is more than the\n'
            s +=f"    default value of 1.9 GB. Given that the overall CPU memory usage {tense} only {cpu_memory_utilization}%,\n"
            s += "    please consider reducing your CPU memory allocation for future jobs. This\n"
            s += "    will reduce your queue times and make the resources available for other\n"
            s += "    users. For more info:\n"
            s +=f"    https://researchcomputing.princeton.edu/support/knowledge-base/memory{self.txt_normal}\n\n"
        if (self.cluster in ["adroit", "della", "tiger"]) and (not self.partition == "datascience") and \
           (gb_per_core > 5) and (cpu_memory_utilization < JobStats.min_memory_usage) and (self.state != "OUT_OF_MEMORY"):
            s +=f'  {self.txt_bold}{self.txt_red}* This job requested {str(gb_per_core).replace(".0", "")} GB of memory per CPU-core which is more than the\n'
            s +=f"    default value of 4 GB. Given that the overall CPU memory usage {tense} only {cpu_memory_utilization}%,\n"
            s += "    please consider reducing your CPU memory allocation for future jobs. This\n"
            s +=f"    will reduce your queue times and make the resources available for other\n"
            s += "    users. For more info:\n"
            s +=f"    https://researchcomputing.princeton.edu/support/knowledge-base/memory{self.txt_normal}\n\n"
        mem_thres_full = JobStats.physics_max_mem if self.account == "physics" else JobStats.cascade_max_mem 
        mem_thres = 0.8 * mem_thres_full  # only complain if less than 80% of max
        if self.cluster == "della" and self.partition == "datascience" and (total_used / 1024**3 < mem_thres):
            s +=f"  {self.txt_bold}{self.txt_red}* This job ran on a large-memory (datascience) node but it only used {total_used / 1024**3:.1f} GB\n"
            s += "    of CPU memory. The large-memory nodes should only be used for jobs that\n"
            s +=f"    require more than {mem_thres_full} GB. For future jobs please use an accurate value\n"
            s += "    for the --mem-per-cpu or --mem directive. For more info:\n"
            s +=f"    https://researchcomputing.princeton.edu/support/knowledge-base/memory{self.txt_normal}\n\n"
        if (int(self.nnodes) > 1) and (int(self.ncpus) / int(self.nnodes) < 14) and ((total_used / 1024**3) / int(self.nnodes) < 128) and (not self.gpus):
            s +=f"  {self.txt_bold}{self.txt_red}* This job used {self.ncpus} CPU-cores over {self.nnodes} nodes. Please try to use as few nodes as\n"
            s += "    possible by increasing the number of cores per node. Run the snodes command\n"
            s += "    to see the number of CPU-cores per node (see CPUS column). For more info:\n"
            s +=f"    https://researchcomputing.princeton.edu/support/knowledge-base/slurm{self.txt_normal}\n\n"
        if (int(self.nnodes) > 1) and (self.gpus > 0):
            if (self.cluster == "della") and (int(self.nnodes) == self.gpus):
                s +=f"  {self.txt_bold}{self.txt_red}* This job used {self.gpus} GPUs over {self.nnodes} nodes. Please try to use as few nodes as\n"
                s += "    possible by allocating more GPUs per node. The GPU nodes on Della have\n"
                s += "    either 2 or 4 GPUs per node. For more info:\n"
                s +=f"    https://researchcomputing.princeton.edu/systems/della{self.txt_normal}\n\n"
            if (self.cluster in ["adroit", "tiger", "traverse"]) and (self.gpus < 4 * int(self.nnodes)):
                s +=f"  {self.txt_bold}{self.txt_red}* This job used {self.gpus} GPUs over {self.nnodes} nodes. Please try to use as few nodes as\n"
                s +=f"    possible by allocating more GPUs per node. The GPU nodes on {self.cluster[0].upper() + self.cluster[1:]} have\n"
                s += "    4 GPUs per node. For more info:\n"
                s +=f"    https://researchcomputing.princeton.edu/support/knowledge-base/gpu-computing{self.txt_normal}\n\n"
        if (self.nnodes == "1") and (self.cluster == "tiger") and (not self.gpus) and (not self.qos == "tiger-test"):
            s +=f"  {self.txt_bold}{self.txt_red}* The TigerCPU cluster is intended for multinode jobs. Serial jobs are\n"
            s += "    assigned the lowest priority. On TigerCPU, a serial job is one that runs\n"
            s += "    on 1 node (independent of the number of cores). Consider carrying out this\n"
            s +=f"    work elsewhere. This note does not apply to GPU jobs.{self.txt_normal}\n\n"
        if (self.nnodes == "1") and (self.cluster == "stellar") and (not self.gpus) and (int(self.ncpus) < 48) and (not self.qos == "stellar-debug"):
            s +=f"  {self.txt_bold}{self.txt_red}* The Stellar cluster is intended for multinode jobs. Serial jobs are assigned\n"
            s += "    the lowest priority. On Stellar, a serial job is one that uses 1 node and\n"
            s += "    less than 48 CPU-cores. Consider carrying out this work elsewhere. This note\n"
            s +=f"    does not apply to GPU jobs.{self.txt_normal}\n\n"
        if self.time_eff_violation:
            s +=f"  {self.txt_bold}{self.txt_red}* The time efficiency of this job is {self.time_efficiency}%. The time efficiency is the run time\n"
            s += "    divided by the time limit. For future jobs please consider decreasing the\n"
            s += "    value of the --time directive to increase the time efficiency. This will\n"
            s += "    lower your queue time and allow the Slurm job scheduler to work more\n"
            s += "    effectively for all users. Jobs with a time limit of less than 62 minutes\n"
            s += "    will land in the test QOS where only a few jobs can run simultaneously. For\n"
            s += "    more info see these pages:\n"
            s +=f"    https://researchcomputing.princeton.edu/support/knowledge-base/slurm\n"
            s +=f"    https://researchcomputing.princeton.edu/support/knowledge-base/job-priority#test-queue{self.txt_normal}\n\n"
        ### black notes ###
        if (not zero_cpu) and (self.cpu_efficiency < JobStats.cpu_utilization_black) and (self.cpu_efficiency >= JobStats.cpu_utilization_red) and \
           (not self.gpus):
            s +=f"  * The overall CPU utilization of this job is {self.cpu_efficiency}%. This value is somewhat low.\n"
            if int(self.ncpus) > 1:
                s += "    Please investigate the reason for the low efficiency. Consider performing a\n"
                s += "    scaling analysis:\n" 
                s += "    https://researchcomputing.princeton.edu/support/knowledge-base/scaling-analysis\n\n"
        if (self.nnodes == "1") and (int(self.ncpus) > 1) and (not self.gpus):
            eff_if_serial = 100 / int(self.ncpus)
            ratio = self.cpu_efficiency / eff_if_serial
            if (ratio > 0.9 and ratio < 1.1):
                s +=f"  * The CPU utilization of this job ({self.cpu_efficiency}%) is approximately equal to 1 divided by\n"
                s +=f"    the number of CPU-cores ({round(100/int(self.ncpus))}%). This suggests that you may be trying to run a\n"
                s += "    serial code using multiple cores.\n\n"
        if (not zero_gpu) and self.gpus and (self.gpu_utilization < JobStats.gpu_utilization_black) and \
           (self.gpu_utilization >= JobStats.gpu_utilization_red):
            s +=f"  * The overall GPU utilization of this job is {round(self.gpu_utilization)}%. This value is low compared\n"
            s += "    to the cluster mean value of about 50%. For more info:\n"
            s += "    https://researchcomputing.princeton.edu/support/knowledge-base/gpu-computing\n\n"
        if "test" in self.qos or "debug" in self.qos:
            s +=f"  * This job ran in the {self.qos} QOS. Each user can only run a small number of\n"
            s += "    jobs simultaneously in this QOS. For more info:\n"
            s += "    https://researchcomputing.princeton.edu/support/knowledge-base/job-priority#test-queue\n\n"
        if self.partition == "mig":
            s +=f"  * This job ran on the mig partition where jobs are limited to 1 CPU-core, 32 GB\n"
            s += "    of CPU memory and 10 GB of GPU memory. A MIG GPU is about 1/7th as powerful as\n"
            s += "    an A100 GPU. Please continue using the mig partition when possible. For more:\n"
            s += "    https://researchcomputing.princeton.edu/systems/della#gpus\n\n"
        ### default ###
        if (self.cluster == "tiger" or self.cluster == "traverse"):
            s += "  * For additional job metrics including metrics plotted against time:\n"
            s += "    https://stats.rc.princeton.edu  (VPN required off-campus)\n\n"
        if (self.cluster == "stellar"):
            s += "  * For additional job metrics including metrics plotted against time:\n"
            s += "    https://mystellar.princeton.edu/pun/sys/jobstats  (VPN required off-campus)\n\n"
        if (self.cluster == "della"):
            s += "  * For additional job metrics including metrics plotted against time:\n"
            s += "    https://mydella.princeton.edu/pun/sys/jobstats  (VPN required off-campus)\n\n"
        if (self.cluster == "adroit"):
            s += "  * For additional job metrics including metrics plotted against time:\n"
            s += "    https://myadroit.princeton.edu/pun/sys/jobstats  (VPN required off-campus)\n\n"
        return s

    def cpu_memory_formatted(self):
        total = self.reqmem.replace("000M", "G").replace("000G", "T").replace(".50G", ".5G").replace(".50T", ".5T")
        if int(self.ncpus) == 1 or all([X not in total for X in ("K", "M", "G", "T")]):
            return f'     CPU Memory: {total.replace("M", "MB").replace("G", "GB").replace("T", "TB")}'
        if total.endswith("K"):
            bytes_ = float(total.replace("K", "")) * 1e3
        elif total.endswith("M"):
            bytes_ = float(total.replace("M", "")) * 1e6
        elif total.endswith("G"):
            bytes_ = float(total.replace("G", "")) * 1e9
        elif total.endswith("T"):
            bytes_ = float(total.replace("T", "")) * 1e12
        else:
            return total
        bytes_per_core = bytes_ / int(self.ncpus)
        for unit in ['B','KB','MB','GB','TB']:
            if bytes_per_core < 1000:
                break
            bytes_per_core /= 1000
        bpc = f"{bytes_per_core:.1f}"
        bpc = bpc.replace(".0", "")
        return f'     CPU Memory: {total.replace("M", "MB").replace("G", "GB").replace("T", "TB")} ({bpc}{unit} per CPU-core)'

    def time_limit_formatted(self):
        self.time_eff_violation = False
        SECONDS_PER_MINUTE = 60
        if self.state == "COMPLETED" and self.timelimitraw > 0:
            self.time_efficiency = round(100 * self.diff / (SECONDS_PER_MINUTE * self.timelimitraw))
            if self.time_efficiency > 100: self.time_efficiency = 100
            clr = ""
            if self.time_efficiency < 50 and self.diff > JobStats.min_runtime_seconds:
                self.time_eff_violation = True
                clr = f"{self.txt_bold}{self.txt_red}"
            return f"     Time Limit: {clr}{self.human_seconds(SECONDS_PER_MINUTE * self.timelimitraw)}{self.txt_normal}"
        else:
            return f"     Time Limit: {self.human_seconds(SECONDS_PER_MINUTE * self.timelimitraw)}"

    def enhanced_output(self):
        print("")
        print(80 * "=")
        print("                              Slurm Job Statistics")
        print(80 * "=")
        print(f"         Job ID: {self.txt_bold}{self.jobid}{self.txt_normal}")
        print(f"  NetID/Account: {self.user}/{self.account}")
        print(f"       Job Name: {self.jobname}")
        if self.state in ("OUT_OF_MEMORY", "TIMEOUT"):
            print(f"          State: {self.txt_bold}{self.txt_red}{self.state}{self.txt_normal}")
        else:
            print(f"          State: {self.state}")
        print(f"          Nodes: {self.nnodes}")
        print(f"      CPU Cores: {self.ncpus}")
        print(self.cpu_memory_formatted())
        if self.gpus:
            print(f"           GPUs: {self.gpus}")
        if self.cluster in ("stellar", "tiger") and (self.partition == "serial"):
            print(f"  QOS/Partition: {self.qos}/{self.txt_bold}{self.txt_red}{self.partition}{self.txt_normal}")
        else:
            print(f"  QOS/Partition: {self.qos}/{self.partition}")
        print(f"        Cluster: {self.cluster}")
        print(f"     Start Time: {self.human_datetime(self.start)}")
        if self.state == "RUNNING":
            print(f"       Run Time: {self.human_seconds(self.diff)} (in progress)")
        else:
            print(f"       Run Time: {self.human_seconds(self.diff)}")
        print(self.time_limit_formatted())
        print("")
        print(f"                              {self.txt_bold}Overall Utilization{self.txt_normal}")
        print(80 * "=")

        def draw_meter(x, hardware, util=False):
          bars = x // 2
          if bars < 0:  bars = 0
          if bars > 50: bars = 50
          text = f"{x}%"
          spaces = 50 - bars - len(text)
          if bars + len(text) > 50:
              bars = 50 - len(text)
              spaces = 0

          clr = clr2 = ""
          if (x < JobStats.cpu_utilization_red and hardware == "cpu" and util and (not self.gpus)) or \
             (x < JobStats.gpu_utilization_red and hardware == "gpu" and util):
              clr  = f"{self.txt_red}"
              clr2 = f"{self.txt_bold}{self.txt_red}"
          return f"{self.txt_bold}[{self.txt_normal}" + clr + bars * "|" + spaces * " " + clr2 + \
                 text + f"{self.txt_normal}{self.txt_bold}]{self.txt_normal}"

        # overall cpu time utilization
        total_used, total, total_cores = self.cpu_util_total__used_alloc_cores
        self.cpu_efficiency = round(100 * total_used / total) if total != 0 else 0
        print("  CPU utilization  " + draw_meter(self.cpu_efficiency, "cpu", util=True))
        # overall cpu memory utilization
        total_used, total, total_cores = self.cpu_mem_total__used_alloc_cores
        cpu_memory_efficiency = round(100 * total_used / total) if total != 0 else 0
        print("  CPU memory usage " + draw_meter(cpu_memory_efficiency, "cpu"))
        if self.gpus:
            # overall gpu utilization
            overall, overall_gpu_count = self.gpu_util_total__util_gpus
            self.gpu_utilization = overall / overall_gpu_count
            if self.partition == "mig":
                unknown = f"  GPU utilization  {self.txt_bold}[{self.txt_normal}" +\
                          f"     GPU utilization is unknown for MIG jobs      " +\
                          f"{self.txt_normal}{self.txt_bold}]{self.txt_normal}"
                print(unknown)
            else:
                print("  GPU utilization  " + draw_meter(round(self.gpu_utilization), "gpu", util=True))
            # overall gpu memory usage
            overall, overall_total = self.gpu_mem_total__used_alloc
            gpu_memory_usage = round(100 * overall / overall_total)
            print("  GPU memory usage " + draw_meter(gpu_memory_usage, "gpu"))
        print()
        print(f"                              {self.txt_bold}Detailed Utilization{self.txt_normal}")
        print(80 * "=")
        self.simple_output()
        print()
        notes = self.job_notes()
        if notes:
            print(f"                                     {self.txt_bold}Notes{self.txt_normal}")
            print(80 * "=")
            print(notes)


    def report_job(self):
        sp_node = self.sp_node

        if len(sp_node)==0:
            if self.diff < 30:
                cmd = ["seff",  f"{self.jobid}"]
                try:
                    seff = subprocess.check_output(cmd, stderr=DEVNULL).decode("utf-8")
                except:
                    self.error("No job statistics are available.")
                else:
                  print("\nRun time is very short so only providing seff output:\n")
                  print(seff)
                  self.error("")
                 
                #self.error("The job %s is too short (%s seconds) for meaningful detailed statistics, please use seff command." % (self.jobid, self.diff))
            else:
                self.error(f"No stats found for job {self.jobid}, either because it is too old or because\n"
                          + "it expired from jobstats database. If you are not running this command on the\n"
                          + "cluster where the job was run then use the -c option to specify the cluster.\n"
                          +f'If the run time was very short then try running "seff {self.jobid}".')

        # cpu utilization
        total = 0
        total_used = 0
        total_cores = 0
        self.cpu_util__node_used_alloc_cores = []
        for n in sp_node:
            used = sp_node[n]['total_time']
            cores = sp_node[n]['cpus']
            alloc = self.diff * cores
            total += alloc
            total_used += used
            total_cores += cores
            self.cpu_util__node_used_alloc_cores.append((n, used, alloc, cores))
        self.cpu_util_total__used_alloc_cores = (total_used, total, total_cores)

        # cpu memory
        total = 0
        total_used = 0
        total_cores = 0
        self.cpu_mem__node_used_alloc_cores = []
        for n in sp_node:
            used = sp_node[n]['used_memory']
            alloc = sp_node[n]['total_memory']
            cores = sp_node[n]['cpus']
            total += alloc
            total_used += used
            total_cores += cores
            self.cpu_mem__node_used_alloc_cores.append((n, used, alloc, cores))
        self.cpu_mem_total__used_alloc_cores = (total_used, total, total_cores)

        if self.gpus:
            # gpu utilization
            overall = 0
            overall_gpu_count = 0
            self.gpu_util__node_util_index = []
            key_found = True
            for n in sp_node:
                d = sp_node[n]
                if 'gpu_utilization' in d:
                    gpus = list(d['gpu_utilization'].keys())
                    gpus.sort()
                    for g in gpus:
                        util = d['gpu_utilization'][g]
                        overall += util
                        overall_gpu_count += 1
                        self.gpu_util__node_util_index.append((n, util, g))
                else:
                    # this branch deals with mig
                    key_found = False
                    self.gpu_util__node_util_index.append((n, 50, "0"))
            if key_found:
                self.gpu_util_total__util_gpus = (overall, overall_gpu_count)
            else:
                # this branch deals with mig
                self.gpu_util_total__util_gpus = (50, 1)

            # gpu memory usage
            overall = 0
            overall_total = 0
            self.gpu_mem__node_used_total_index = []
            for n in sp_node:
                d = sp_node[n]
                gpus = list(d['gpu_total_memory'].keys())
                gpus.sort()
                for g in gpus:
                    used = d['gpu_used_memory'][g]
                    total = d['gpu_total_memory'][g]
                    overall += used
                    overall_total += total
                    self.gpu_mem__node_used_total_index.append((n, used, total, g))
            self.gpu_mem_total__used_alloc = (overall, overall_total)

        self.simple_output() if self.simple else self.enhanced_output()

 
    def __str__(self, compact=False):
        js_data = { 'nodes': self.sp_node, 'total_time': self.diff, 'gpus': self.gpus }
        if compact:
            return json.dumps(js_data, separators=(',', ':'))
        else:
            return json.dumps(js_data, sort_keys=True, indent=4)

    def report_job_json(self, encode):
        data = self.__str__(encode)
        if encode:
            if self.diff < 60:
                return 'Short'
            elif len(self.sp_node)==0:
                return 'None'
            else:
                return base64.b64encode(gzip.compress(data.encode('ascii'))).decode('ascii')
        else:
            return data

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Show job utilization.")
    parser.add_argument('job', metavar='jobid', nargs='+',
                    help='Job numbers to lookup')
    parser.add_argument("-c", "--cluster", default=None,
                    help="Specify cluster instead of relying on default on the current machine.")
    parser.add_argument("-j", "--json", action='store_true', default=False,
                    help="Produce row data in json format, with no summary.")
    parser.add_argument("-b", "--base64", action='store_true', default=False,
                    help="Produce row data in json format, with no summary and also gzip and encode it in base64 output for db storage.")
    parser.add_argument("-d", "--debug", action='store_true', default=False,
                    help="Output debugging information.")
    parser.add_argument("-S", "--syslog", action='store_true', default=False,
                    help="Output debugging information to syslog.")
    parser.add_argument("-s", "--simple", action='store_true', default=False,
                    help="Output information using a simple format.")
    parser.add_argument("-f", "--force", action='store_true', default=False,
                    help="Force recalculation without using cached data from the database.")
    parser.add_argument("-n", "--no-color", action='store_true', default=False,
                    help="Output information without colorization.")
    args = parser.parse_args()

    term = Terminal()
    txt_bold   = f"{term.bold}"
    txt_red    = f"{term.red}"
    txt_normal = f"{term.normal}"
    color = ("","","") if args.no_color else (txt_bold, txt_red, txt_normal)

    if args.cluster == "tiger": args.cluster = "tiger2"
    for jobid in args.job:
        #import pdb; pdb.set_trace()
        stats = JobStats(jobid=jobid,
                         cluster=args.cluster,
                         debug=args.debug,
                         debug_syslog=args.syslog,
                         force_recalc=args.force,
                         simple=args.simple,
                         color=color)
        if args.json or args.base64:
            print(stats.report_job_json(args.base64))
        else:
            stats.report_job()
