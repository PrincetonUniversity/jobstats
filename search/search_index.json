{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"What is Jobstats?","text":"<p>Jobstats is a free and open-source job monitoring platform designed for CPU and GPU clusters that use the Slurm workload manager. It was released in 2023 under the GNU GPL v2 license. Visit the Jobstats GitHub repository.</p>"},{"location":"#what-are-the-main-benefits-of-jobstats-over-other-platforms","title":"What are the main benefits of Jobstats over other platforms?","text":"<p>The main advantages of Jobstats are:</p> <ul> <li>no action is required by the user </li> <li>utilization and memory usage for each allocated GPU</li> <li>automatically cancel jobs with 0% GPU utilization (more info)</li> <li>accurate CPU memory usage for jobs of any size</li> <li>can be used for completed and actively running jobs</li> <li>graphical interface for inspecting job metrics versus time</li> <li>efficiency reports contain job-specific notes to guide users</li> <li>automated emails to users for instances of underutilization (more info)</li> <li>periodic reports on usage and efficiency for users and group leaders</li> <li>all of the above features work with Open OnDemand jobs</li> </ul>"},{"location":"#how-does-jobstats-work","title":"How does Jobstats work?","text":"<p>A schematic diagram of the components of the Jobstats platform and the external tools is shown below:</p> <p></p> <p>A compute node with two sockets is shown in the upper left. The dotted line around the node indicates the three node-level exporters, namely, Node, cgroups and NVIDIA. A GPFS server is shown in the upper right with its cluster-level GPFS exporter. The exporters serve to make data available to the Prometheus database. Users interact with the Prometheus and Slurm data via the web interface (i.e., Grafana) and external tools (e.g., <code>jobstats</code>).</p>"},{"location":"#which-institutions-are-using-jobstats","title":"Which institutions are using Jobstats?","text":"<p>Jobstats is used by these institutions:</p> <ul> <li>American Museum of Natural History</li> <li>Brown University - Center for Computation and Visualization</li> <li>Clemson University - Research Computing and Data</li> <li>Deakin University - Applied Artificial Intelligence Initiative</li> <li>Free University of Berlin - High-Performance Computing</li> <li>George Mason University - Office of Research Computing</li> <li>Harvard University - Research Computing</li> <li>Johns Hopkins University - Advanced Research Computing</li> <li>Iowa State University - High Performance Computing</li> <li>Monash University - e-Research</li> <li>Northwestern University - Research Computing and Data Services</li> <li>NAVER LABS Europe</li> <li>Pacific Northwest National Laboratory</li> <li>Princeton University - Computer Science Department</li> <li>Princeton University - Research Computing</li> <li>University of Queensland - Research Computing Centre</li> <li>University of Virginia - Research Computing</li> <li>Yale University - Center for Research Computing</li> <li>and many more</li> </ul>"},{"location":"#what-does-a-jobstats-efficiency-report-look-like","title":"What does a Jobstats efficiency report look like?","text":"<p>The <code>jobstats</code> command generates a job report:</p> <pre><code>$ jobstats 39798795\n\n================================================================================\n                              Slurm Job Statistics\n================================================================================\n         Job ID: 39798795\n   User/Account: aturing/math\n       Job Name: sys_logic_ordinals\n          State: COMPLETED\n          Nodes: 2\n      CPU Cores: 48\n     CPU Memory: 256GB (5.3GB per CPU-core)\n           GPUs: 4\n  QOS/Partition: della-gpu/gpu\n        Cluster: della\n     Start Time: Fri Mar 4, 2022 at 1:56 AM\n       Run Time: 18:41:56\n     Time Limit: 4-00:00:00\n\n                              Overall Utilization\n================================================================================\n  CPU utilization  [|||||                                          10%]\n  CPU memory usage [|||                                             6%]\n  GPU utilization  [||||||||||||||||||||||||||||||||||             68%]\n  GPU memory usage [|||||||||||||||||||||||||||||||||              66%]\n\n                              Detailed Utilization\n================================================================================\n  CPU utilization per node (CPU time used/run time)\n      della-i14g2: 1-21:41:20/18-16:46:24 (efficiency=10.2%)\n      della-i14g3: 1-18:48:55/18-16:46:24 (efficiency=9.5%)\n  Total used/runtime: 3-16:30:16/37-09:32:48, efficiency=9.9%\n\n  CPU memory usage per node - used/allocated\n      della-i14g2: 7.9GB/128.0GB (335.5MB/5.3GB per core of 24)\n      della-i14g3: 7.8GB/128.0GB (334.6MB/5.3GB per core of 24)\n  Total used/allocated: 15.7GB/256.0GB (335.1MB/5.3GB per core of 48)\n\n  GPU utilization per node\n      della-i14g2 (GPU 0): 65.7%\n      della-i14g2 (GPU 1): 64.5%\n      della-i14g3 (GPU 0): 72.9%\n      della-i14g3 (GPU 1): 67.5%\n\n  GPU memory usage per node - maximum used/total\n      della-i14g2 (GPU 0): 26.5GB/40.0GB (66.2%)\n      della-i14g2 (GPU 1): 26.5GB/40.0GB (66.2%)\n      della-i14g3 (GPU 0): 26.5GB/40.0GB (66.2%)\n      della-i14g3 (GPU 1): 26.5GB/40.0GB (66.2%)\n\n                                     Notes\n================================================================================\n  * This job only used 6% of the 256GB of total allocated CPU memory. For\n    future jobs, please allocate less memory by using a Slurm directive such\n    as --mem-per-cpu=1G or --mem=10G. This will reduce your queue times and\n    make the resources available to other users. For more info:\n      https://researchcomputing.princeton.edu/support/knowledge-base/memory\n\n  * See the URL below for various job metrics plotted as a function of time:\n      https://mydella.princeton.edu/pun/sys/jobstats/39798795\n</code></pre>"},{"location":"#which-metrics-does-jobstats-make-available","title":"Which metrics does Jobstats make available?","text":"<p>Job-level metrics:</p> <ul> <li>CPU Utilization</li> <li>CPU Memory Utilization</li> <li>GPU Utilization</li> <li>GPU Memory\u00a0Utilization</li> <li>GPU Power Usage</li> <li>GPU Temperature</li> </ul> <p>Node-level metrics:</p> <ul> <li>CPU Percentage Utilization</li> <li>Total Memory Utilization</li> <li>Mean Frequency Over All CPUs</li> <li>NFS Statistics</li> <li>Local Disc R/W</li> <li>GPFS Bandwidth Statistics</li> <li>Local Disc IOPS</li> <li>GPFS Operations per Second Statistics</li> <li>Infiniband Throughput</li> <li>Infiniband Packet Rate</li> <li>Infiniband Errors</li> </ul> <p>The following image shows the Grafana dashboard for an example GPU job:</p> <p></p>"},{"location":"#other-job-monitoring-platforms","title":"Other job monitoring platforms","text":"<p>Consider these alternatives to Jobstats:</p> <ul> <li>REMORA</li> <li>XDMod (SUPReMM)</li> <li>HPCPerfStats</li> <li>LLload</li> <li>jobperf</li> <li>Performance Co-Pilot</li> </ul>"},{"location":"#want-to-use-jobstats-at-your-institution","title":"Want to use Jobstats at your institution?","text":"<p>Proceed to the next section where we illustrate the setup of the platform.</p>"},{"location":"contributions/","title":"Contributions","text":"<p>Contributions to the Jobstats platform and its tools are welcome. To work with the code, build a Conda environment:</p> <pre><code>$ conda create --name jobstats-dev requests blessed ruff pytest-mock mkdocs-material -c conda-forge\n</code></pre> <p>Be sure that the tests are passing before making a pull request:</p> <pre><code>(jobstats-dev) $ pytest\n</code></pre>"},{"location":"contributions/#list-of-contributors","title":"List of Contributors","text":"<ul> <li>Anish Chanda, Iowa State University, provided the option of using an external MariaDB database to store the job summary statistics.</li> </ul>"},{"location":"publications/","title":"Publications and Presentations","text":"<p>Combating Underutilization with the Jobstats Job Monitoring Platform PEARC 2025, Columbus, Ohio (July 23, 2025) Poster: PDF \u00a0 | \u00a0 ACM: HTML</p> <p>Jobstats at the Campus Research Computing Consortium (CaRCC) Topic: Efficient Usage of Compute Resources Virtual Event (July 10, 2025) Slides: PDF</p> <p>Integration of Open OnDemand with the Jobstats Job Monitoring Platform Global Open OnDemand Conference, Harvard University (March 19, 2025) Slides: PDF</p> <p>Lessons Learned from 8 Years of Open OnDemand at Princeton Research Computing Global Open OnDemand Conference, Harvard University (March 18, 2025) Slides: PDF \u00a0 | \u00a0 Video: Vimeo</p> <p>Jobstats: A Slurm-Compatible Job Monitoring Platform for CPU and GPU Clusters PEARC 2024, Providence, RI (July 24, 2024) Poster: PDF </p> <p>Jobstats: A Slurm-Compatible Job Monitoring Platform for CPU and GPU Clusters PEARC 2023, Portland, OR (July 25, 2023) Slides: PDF </p> <p>Jobstats: A Slurm-Compatible Job Monitoring Platform for CPU and GPU Clusters PEARC '23: Practice and Experience in Advanced Research Computing 2023: Computing for the Common Good, Association for Computing Machinery, Pages 102-108, Year 2023 Paper: PDF </p> <p> </p>"},{"location":"publications/#awards","title":"Awards","text":"<p>Jobstats won the \"Best Poster Award\" at PEARC 2025.</p>"},{"location":"support/","title":"Support","text":"<p>For assistance with setting up the Jobstats platform, please post an issue on the Jobstats GitHub repository: https://github.com/PrincetonUniversity/jobstats</p>"},{"location":"support/#join-the-mailing-list","title":"Join the Mailing List","text":"<p>Subscribe to the Jobstats mailing list to receive updates and other news.</p>"},{"location":"setup/cgroups/","title":"CPU Job Statistics","text":"<p>Slurm has to be configured to track job accounting data via the cgroup plug-in. This requires the following line in <code>slurm.conf</code>:</p> <pre><code>JobAcctGatherType=jobacct_gather/cgroup\n</code></pre> <p>The above is in addition to the other usual cgroup-related plug-ins/settings:</p> <pre><code>ProctrackType=proctrack/cgroup\nTaskPlugin=affinity,cgroup\n</code></pre> <p>Slurm will then create two top-level cgroup directories for each job, one for CPU utilization and one for CPU memory. Within each directory there will be subdirectories: <code>step_extern</code>, <code>step_batch</code>, <code>step_0</code>, <code>step_1</code>, and so on. Within these directories one finds <code>task_0</code>, <code>task_1</code>, and so on. These cgroups are scraped by a cgroup exporter. The table below lists all of the collected fields:</p> Name Description Type <code>cgroup_cpu_system_seconds</code> Cumulative CPU system seconds for jobid gauge <code>cgroup_cpu_total_seconds</code> Cumulative CPU total seconds for jobid gauge <code>cgroup_cpu_user_seconds</code> Cumulative CPU user seconds for jobid gauge <code>cgroup_cpus</code> Number of CPUs in the jobid gauge <code>cgroup_memory_cache_bytes</code> Memory cache used in bytes gauge <code>cgroup_memory_fail_count</code> Memory fail count gauge <code>cgroup_memory_rss_bytes</code> Memory RSS used in bytes gauge <code>cgroup_memory_total_bytes</code> Memory total given to jobid in bytes gauge <code>cgroup_memory_used_bytes</code> Memory used in bytes gauge <code>cgroup_memsw_fail_count</code> Swap fail count gauge <code>cgroup_memsw_total_bytes</code> Swap total given to jobid in bytes gauge <code>cgroup_memsw_used_bytes</code> Swap used in bytes gauge <code>cgroup_uid</code> UID number of user running this job gauge <p>The cgroup exporter used here is based on the exporter by Trey Dock [1] with additional parsing of the <code>jobid</code>, <code>steps</code>, <code>tasks</code> and <code>UID</code> number. This produces an output that resembles (e.g., for system seconds):</p> <pre><code>cgroup_cpu_system_seconds{jobid=\"247463\", step=\"batch\", task=\"0\"}\n160.92\n</code></pre> <p>Note that the UID of the owning user is stored as a gauge in <code>cgroup_uid</code>:</p> <pre><code>cgroup_uid{jobid=\"247463\"}\n334987\n</code></pre> <p>This is because accounting is job-oriented and having a UID of the user as a label would needlessly increase the cardinality of the data in Prometheus. All other fields are alike with <code>jobid</code>, <code>step</code> and <code>task</code> labels.</p> <p>The totals for a job have an empty <code>step</code> and <code>task</code>, for example:</p> <pre><code>cgroup_cpu_user_seconds{jobid=\"247463\", step=\"\", task=\"\"}\n202435.71\n</code></pre> <p>This is due to the organization of the cgroup hierarchy. Consider the directory:</p> <pre><code>/sys/fs/cgroup/cpu,cpuacct/slurm/uid_334987\n</code></pre> <p>Within this directory, one finds the following subdirectories:</p> <pre><code>job_247463/cpuacct.usage_user\njob_247463/step_extern/cpuacct.usage_user\njob_247463/step_extern/task_0/cpuacct.usage_user\n</code></pre> <p>This is the data most often retrieved and parsed for overall job efficiency which is why by default the <code>cgroup_exporter</code> does not parse <code>step</code> or <code>task</code> data. To collect all of it, add the <code>--collect.fullslurm option</code>. We run the <code>cgroup_exporter</code> with these options:</p> <pre><code>/usr/sbin/cgroup_exporter --config.paths /slurm --collect.fullslurm\n</code></pre> <p>The <code>--config.paths /slurm</code> has to match the path used by Slurm under the top cgroup directory. This is usually a path that is something like <code>/sys/fs/cgroup/memory/slurm</code>.</p>"},{"location":"setup/external-database/","title":"External Database Configuration","text":"<p>This section describes how to configure Jobstats to use an external MariaDB/MySQL database instead of storing job summary statistics in the `AdminComment field of the Slurm database.</p>"},{"location":"setup/external-database/#overview","title":"Overview","text":"<p>By default, Jobstats stores job statistics in the Slurm database by updating the <code>AdminComment</code> field in the job table. The feature described here allows for storing the statistics in a separate external MariaDB/MySQL database instead. This is useful for:</p> <ul> <li>Separating Jobstats data from the Slurm database</li> <li>Easier data analysis and reporting</li> <li>Database backup and maintenance flexibility</li> </ul>"},{"location":"setup/external-database/#configuration","title":"Configuration","text":""},{"location":"setup/external-database/#1-database-setup","title":"1. Database Setup","text":"<p>First, create a MariaDB/MySQL database and table to store the job statistics:</p> <pre><code>CREATE DATABASE jobstats;\nUSE jobstats;\n\nCREATE TABLE job_statistics (\n    id INT AUTO_INCREMENT PRIMARY KEY,\n    cluster VARCHAR(50) NOT NULL,\n    jobid VARCHAR(50) NOT NULL,\n    admin_comment TEXT,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,\n    UNIQUE KEY unique_cluster_job (cluster, jobid)\n);\n</code></pre>"},{"location":"setup/external-database/#2-python-dependencies","title":"2. Python Dependencies","text":"<p>Install the required MySQL client library:</p> <pre><code># For conda environments\nconda install mysqlclient\n\n# Or using pip\npip install mysqlclient\n</code></pre>"},{"location":"setup/external-database/#3-configuration-file","title":"3. Configuration File","text":"<p>Edit <code>config.py</code> to enable external database support:</p> <pre><code>EXTERNAL_DB_CONFIG = {\n    \"enabled\": True,  # Set to True to enable external DB\n    \"host\": \"your-database-host\",\n    \"port\": 3306,\n    \"database\": \"jobstats\",\n    \"user\": \"jobstats_user\",\n    \"password\": \"your_password\",\n    # Alternatively, use a MySQL config file:\n    # \"config_file\": \"/path/to/mysql.cnf\"\n}\n</code></pre>"},{"location":"setup/external-database/#using-mysql-configuration-file-recommended","title":"Using MySQL Configuration File (Recommended)","text":"<p>For better security, one can use a MySQL configuration file instead of hardcoding the credentials:</p> <pre><code>EXTERNAL_DB_CONFIG = {\n    \"enabled\": True,\n    \"database\": \"jobstats\",\n    \"config_file\": \"/etc/jobstats/mysql.cnf\"\n}\n</code></pre> <p>Create the MySQL config file (<code>/etc/jobstats/mysql.cnf</code>): <pre><code>[client]\nhost = your-database-host\nport = 3306\nuser = jobstats_user\npassword = your_password\n</code></pre></p>"},{"location":"setup/external-database/#4-script-installation","title":"4. Script Installation","text":"<p>Copy the <code>store_jobstats.py</code> script to <code>/usr/local/bin/</code> on your Slurm controller:</p> <pre><code>sudo cp store_jobstats.py /usr/local/bin/\nsudo chmod +x /usr/local/bin/store_jobstats.py\n</code></pre>"},{"location":"setup/external-database/#5-slurm-configuration","title":"5. Slurm Configuration","text":"<p>Update your <code>slurmctldepilog.sh</code> script. The script will automatically detect the presence of <code>store_jobstats.py</code> and use external database storage when available.</p>"},{"location":"setup/external-database/#how-it-works","title":"How It Works","text":""},{"location":"setup/external-database/#storage-behavior","title":"Storage Behavior","text":"<ul> <li>External DB enabled: Job statistics are stored only in the external database</li> <li>External DB disabled: Job statistics are stored in <code>AdminComment</code> in Slurm DB (default behavior)</li> </ul>"},{"location":"setup/external-database/#epilog-script-logic","title":"Epilog Script Logic","text":"<p>The <code>slurmctldepilog.sh</code> script uses the following conditional logic:</p> <p>\u00a0\u00a0\u00a0\u00a0If <code>/usr/local/bin/store_jobstats.py</code> exists:</p> <ul> <li>store jobstats in external database only</li> <li>log success/failure for the attempt</li> </ul> <p>\u00a0\u00a0\u00a0\u00a0If <code>/usr/local/bin/store_jobstats.py</code> does NOT exist:</p> <ul> <li>use traditional Slurm <code>AdminComment</code> storage (maintains backward compatibility)</li> </ul> <p>This ensures that:</p> <ul> <li>systems without external DB setup continue to work normally  </li> <li>systems with external DB use only the external database (no fallback)</li> </ul>"},{"location":"setup/external-database/#data-retrieval","title":"Data Retrieval","text":"<p>When using the <code>jobstats</code> command:</p> <ul> <li>the Slurm <code>AdminComment</code> field is checked for compatibility with existing data</li> <li>if no data found and external DB is enabled then retrieve from external database</li> </ul>"},{"location":"setup/external-database/#migration","title":"Migration","text":"<p>From Slurm <code>AdminComment</code> to External DB:</p> <ol> <li>Set up the external database and configure <code>config.py</code></li> <li>Install the <code>store_jobstats.py</code> script</li> <li>Future jobs will automatically use the external database</li> </ol>"},{"location":"setup/external-database/#troubleshooting","title":"Troubleshooting","text":"<p>Common issues:</p> <ol> <li>MySQLdb import error: Install <code>mysqlclient</code> package</li> <li>Connection failed: Check database credentials and network connectivity</li> <li>Permission denied: Ensure <code>store_jobstats.py</code> is executable</li> <li>Storage handler failed: Check database permissions and table existence</li> </ol>"},{"location":"setup/gpu_node_scripts/","title":"GPU Job Statistics","text":"<p>GPU metrics (currently only NVIDIA) are collected by the Jobstats GPU exporter which was based on the exporter by Rohit Agarwal [1]. The main local changes were to add the handling of Multi-Instance GPUs (MIG) and two additional gauge metrics: <code>nvidia_gpu_jobId</code> and <code>nvidia_gpu_jobUid</code>. The table below lists all of the collected GPU fields.</p> Name Description Type <code>nvidia_gpu_duty_cycle</code> GPU utilization gauge <code>nvidia_gpu_memory_total_bytes</code> Total memory of the GPU device in bytes gauge <code>nvidia_gpu_memory_used_bytes</code> Memory used by the GPU device in bytes gauge <code>nvidia_gpu_num_devices</code> Number of GPU devices gauge gauge <code>nvidia_gpu_power_usage_milliwatts</code> Power usage of the GPU device in milliwatts gauge <code>nvidia_gpu_temperature_celsius</code> Temperature of the GPU device in Celsius gauge <code>nvidia_gpu_jobId</code> JobId number of a job currently using this GPU as reported by Slurm gauge <code>nvidia_gpu_jobUid</code> UID number of user running jobs on this GPU gauge <p>Note</p> <p>Note that the approach described here is not appropriate for clusters that allow for GPU sharing (e.g., sharding).</p>"},{"location":"setup/gpu_node_scripts/#gpu-job-ownership-helper","title":"GPU Job Ownership Helper","text":"<p>In order to correctly track which GPU is assigned to which <code>jobid</code>, we use Slurm prolog and epilog scripts to create files in <code>/run/gpustat</code>. These files are named either after GPU ordinal number (0, 1, ...) or, in the case of Multi-Instance GPUs (MIG), MIG-UUID. These files contain the space-separated <code>jobid</code> and <code>UID</code> number of the user, for example:</p> <pre><code>$ cat /run/gpustat/MIG-265a219d-a49f-578a-825d-222c72699c16\n45916256 262563\n</code></pre> <p>These two scripts can be found in the <code>slurm</code> directory of the Jobstats GitHub repository. For example, <code>slurm/epilog.d/gpustats_helper.sh</code> could be installed as <code>/etc/slurm/epilog.d/gpustats_helper.sh</code> and <code>slurm/prolog.d/gpustats_helper.sh</code> as <code>/etc/slurm/prolog.d/gpustats_helper.sh</code> with these <code>slurm.conf</code> statements:</p> <pre><code>Prolog=/etc/slurm/prolog.d/*.sh\nEpilog=/etc/slurm/epilog.d/*.sh\n</code></pre> <p>For efficiency and simplicity, <code>JobId</code> and <code>jobUid</code> are collected from files in either '/run/gpustat/UUID-OF-THE-GPU<code>(e.g.</code>/run/gpustat/GPU-03aa20a1-2e97-d125-629d-fd4e5734553f<code>) or</code>/run/gpustat/0<code>(for GPU 0),</code>/run/gpustat/1` (for GPU 1), and so on. For example:</p> <pre><code>$ cat /run/gpustat/0\n247609 223456\n</code></pre> <p>In the above, the first number is the <code>jobid</code> and the second is the <code>UID</code> number of the owning user. These are created with Slurm <code>prolog.d</code> and <code>epilog.d</code> scripts that can be found in the Jobstats GitHub repository.</p>"},{"location":"setup/gpu_node_scripts/#gpu-ownership-caveats","title":"GPU Ownership Caveats","text":"<p>Depending on your slurm gres configuration you may need to change either prolog/epilog scripts or gres.conf in order to match correctly GPUs to jobs running on them. The problem is in the difference between GPU ordinal number and its minor number. Slurm's gres.conf can make CUDA_VISIBLE_DEVICES in prolog/epilog scripts be the (recommended) ordinal number, the minor number or even something entirely different.</p> <p>The ordinal number corresponds to the order of how GPUs are displayed in the nvidia-smi output where they are sorted by their PCI/BUS-id number (lowest one is #0, and so on). This is also the number you have to use in CUDA_VISIBLE_DEVICES or as a parameter to <code>nvidia-smi -i</code> option.</p> <p>A GPU minor number corresponds to its <code>/dev/nvidiaX</code> number. One way to see GPU minor numbers would be to run the following:</p> <pre><code># grep 'Device Minor' /proc/driver/nvidia/gpus/*/information\n/proc/driver/nvidia/gpus/0000:07:00.0/information:Device Minor:      2\n/proc/driver/nvidia/gpus/0000:0b:00.0/information:Device Minor:      3\n/proc/driver/nvidia/gpus/0000:48:00.0/information:Device Minor:      0\n/proc/driver/nvidia/gpus/0000:4c:00.0/information:Device Minor:      1\n/proc/driver/nvidia/gpus/0000:88:00.0/information:Device Minor:      6\n/proc/driver/nvidia/gpus/0000:8b:00.0/information:Device Minor:      7\n/proc/driver/nvidia/gpus/0000:c8:00.0/information:Device Minor:      4\n/proc/driver/nvidia/gpus/0000:cb:00.0/information:Device Minor:      5\n</code></pre> <p>The above is the output from a GPU node where ordinal number does not match its GPU minor number and where GPU#0 is <code>/dev/nvidia2</code> and not <code>/dev/nvidia0</code>.</p> <p>How can this cause a problem and incorrect stats? Let's assume a job that requests a single GPU and slurm allocates it the GPU#0, on a node where ordinal!=minor, as illustrated above.</p> <p>If you configured your GPU nodes with gres.conf that contains <code>AutoDetect=nvml</code> slurm will, through its use of nvidia libraries, give you the GPU with PCI-ID <code>0000:07:00.0</code> with ordinal number 0 and <code>/dev/nvidia2</code> device. In <code>/etc/slurm/prolog/epilog</code> scripts <code>CUDA_VISIBLE_DEVICES</code> is set to 0 and <code>nvidia-smi -i 0</code> will fetch the correct UUID and therefore properly set the <code>/run/gpustat/0</code> and <code>/run/gpustat/GPU-UUID</code> files. User's job will be given access (via devices cgroup) to the <code>/dev/nvidia2</code> - so everything matches and therefore status will match the actual utilization.</p> <p>On the other hand, when gres.conf is configured as follows:</p> <pre><code>Name=gpu Type=a100 Count=8 File=/dev/nvidia[0-7]\n</code></pre> <p>slurm will assume that GPU#0 is <code>/dev/nvidia0</code> (GPU#1 is <code>/dev/nvidia1</code>, ...).  In <code>/etc/slurm/prolog/epilog</code> scripts slurm will still set <code>CUDA_VISIBLE_DEVICES</code> to 0 but now as far as <code>nvidia-smi -i 0</code> that's actually the GPU with <code>/dev/nvidia2</code> which will not match what the user will be given access to in the job (<code>/dev/nvidia0</code>). Hence the incorrect data collection.</p> <p>One obvious fix would be to stick with <code>AutoDetect=nvml</code>. Another would be to not attempt to use UUID's for data collection, that is to remove</p> <pre><code>  if [ \"${i:0:3}\" != \"MIG\" ]; then\n    UUID=\"`/usr/bin/nvidia-smi --query-gpu=uuid --format=noheader,csv -i $i`\"\n    echo $SLURM_JOB_ID $SLURM_JOB_UID &gt; \"$DEST/$UUID\"\n  fi\n</code></pre> <p>which will make it rely only on the numbers in <code>CUDA_VISIBLE_DEVICES</code>. Note that this relies on using a recent version of our <code>nvidia_gpu_exporter</code> that defaults to using minor numbers in <code>/run/gpustats</code>.</p> <p>You could also set gres.conf by hand correctly:</p> <pre><code>Name=gpu Type=a100 Count=8 File=/dev/nvidia2,/dev/nvidia3,/dev/nvidia0,/dev/nvidia1,/dev/nvidia6,/dev/nvidia7,/dev/nvidia4,/dev/nvidia5\n</code></pre> <p>Finally, assuming <code>File=/dev/nvidia[0-7]</code> config (so use of minor numbers) you can figure out the ordinal number in the script, e.g.:</p> <pre><code>#!/bin/bash\nDEST=/run/gpustat\n[ -e $DEST ] || mkdir -m 755 $DEST\nfor i in ${GPU_DEVICE_ORDINAL//,/ } ${CUDA_VISIBLE_DEVICES//,/ }; do\n  if [ \"${i:0:3}\" != \"MIG\" ]; then\n    PCI_ID=`egrep \"Device Minor:[[:space:]]+${i}$\" /proc/driver/nvidia/gpus/*/information|cut -d/ -f6`\n    UUID=`/usr/bin/nvidia-smi --query-gpu=uuid --format=noheader,csv -i $PCI_ID`\n    INDEX=`/usr/bin/nvidia-smi --query-gpu=index --format=noheader,csv -i $PCI_ID`\n    echo $SLURM_JOB_ID $SLURM_JOB_UID &gt; \"$DEST/$INDEX\"\n    echo $SLURM_JOB_ID $SLURM_JOB_UID &gt; \"$DEST/$UUID\"\n  else\n  echo $SLURM_JOB_ID $SLURM_JOB_UID &gt; $DEST/$i\n  fi\ndone\nexit 0\n</code></pre>"},{"location":"setup/grafana/","title":"Grafana","text":"<p>The four exporters lead to a wealth of data in the Prometheus database. To visualize this data, the Grafana visualization toolkit is used. To set up Grafana follow the directions at grafana.com.</p> <p>The Grafana dashboard JSON file, which uses all of the exporters, is included in the <code>grafana</code> subdirectory in the Jobstats GitHub repository. The dashboard expects one parameter, <code>jobid</code>. As it may not be easy to find the time range of the job, we also use an OnDemand helper that generates the correct time range given a <code>jobid</code> (see the next section).</p> <p>The following job-level metrics are available in both Grafana and the <code>jobstats</code> command:</p> <ul> <li>CPU Utilization</li> <li>CPU Memory Utilization</li> <li>GPU Utilization </li> <li>GPU Memory\u00a0Utilization </li> </ul> <p>The following additional job-level metrics are exposed only in Grafana:</p> <ul> <li>GPU Power Usage</li> <li>GPU Temperature </li> </ul> <p>Finally, the following additional node-level metrics are exposed only in Grafana:</p> <ul> <li>CPU Percentage Utilization</li> <li>Total Memory Utilization</li> <li>Mean Frequency Over All CPUs</li> <li>NFS Statistics</li> <li>Local Disc R/W</li> <li>GPFS Bandwidth Statistics</li> <li>Local Disc IOPS</li> <li>GPFS Operations per Second Statistics </li> <li>Infiniband Throughput</li> <li>Infiniband Packet Rate</li> <li>Infiniband Errors</li> </ul> <p>The complete Grafana interface for the Jobstats platform is composed of plots of the time history of the seventeen quantities above. This graphical interface is used for detailed investigations such as troubleshooting failed jobs, identifying jobs with CPU memory leaks, intermittent GPU usage, load imbalance, and for understanding the anomalous behavior of system hardware.</p> <p>Note</p> <p>Eleven of the seventeen metrics above are node-level. This means that if multiple jobs are running on the same node then it will not be possible to disentangle the data. To use these metrics to troubleshoot jobs, the job should allocate the entire node.</p> <p>The following image illustrates what the dashboard looks like in use:</p> <p></p>"},{"location":"setup/jobstats_command/","title":"The <code>jobstats</code> command","text":"<p>The last step in setting up the Jobstats platform is installing the <code>jobstats</code> command. This command generates the job efficiency report. For completed jobs, the data is available in the Slurm (or MariaDB) database. For actively running jobs, the Prometheus database must be queried to obtain the data needed to generate the report.</p>"},{"location":"setup/jobstats_command/#installation","title":"Installation","text":"<p>The installation requirements for <code>jobstats</code> are Python 3.6+, Requests 2.20+ and (optionally) blessed 1.17+ which can be used for coloring and styling text. If MariaDB is used instead of the Slurm database then <code>mysqlclient</code> will be needed.</p> <p>The necessary software can be installed as follows:</p> Ubuntucondapip <pre><code>$ apt-get install python3-requests python3-blessed\n</code></pre> <pre><code>$ conda create --name js-env python=3.7 requests blessed -c conda-forge\n</code></pre> <pre><code>$ python3 -m venv .venv\n$ source .venv/bin/activate\n(.venv) $ pip3 install requests blessed\n</code></pre> <p>The four files needed to run the <code>jobstats</code> command are available in the Jobstats GitHub repository.</p> <p>First, store the files in a path such as:</p> <pre><code>$ ls /usr/local/jobstats\nconfig.py\njobstats\njobstats.py\noutput_formatters.py\n</code></pre> <p>Then create a symlink in <code>/usr/local/bin</code> pointing to the executable:</p> <pre><code>$ ln -s /usr/local/jobstats/jobstats /usr/local/bin/jobstats\n</code></pre> <p>Remember to change the permissions to make <code>jobstats</code> executable. An overview of the command can be seen by looking at the help menu:</p> <pre><code>$ jobstats --help\n</code></pre> <p>The command takes the <code>jobid</code> as the only required argument:</p> <pre><code>$ jobstats 12345678\n</code></pre>"},{"location":"setup/jobstats_command/#configuration-file","title":"Configuration File","text":"<p>The <code>jobstats</code> command requires a <code>config.py</code> configuration file. Use <code>config.py</code> in the Jobstats GitHub repository as the starting point for your configuration.</p> <p>The first entry in <code>config.py</code> is for the Prometheus server:</p> <pre><code># prometheus server address, port, and retention period\nPROM_SERVER = \"http://cluster-stats:8480\"\nPROM_RETENTION_DAYS = 365\n</code></pre> <p><code>PROM_RETENTION_DAYS</code> is the number of days that job data will remain the Prometheus database. This is used in deciding whether to display the Grafana URL for a given job as a custom note in the <code>jobstats</code> output.</p> <p>Job summary statistics can be stored in the Slurm database or one can use an external MariaDB database. By default, Slurm DB will be used:</p> <pre><code># if using Slurm database then include the lines below with \"enabled\": False\n# if using MariaDB then set \"enabled\": True\nEXTERNAL_DB_TABLE = \"job_statistics\"\nEXTERNAL_DB_CONFIG = {\n    \"enabled\": False,  # set to True to use the external db for storing stats\n    \"host\": \"127.0.0.1\",\n    \"port\": 3307,\n    \"database\": \"jobstats\",\n    \"user\": \"jobstats\",\n    \"password\": \"password\",\n#     \"config_file\": \"/path/to/jobstats-db.cnf\"\n}\n</code></pre> <p>If you wish to use MariaDB then see External Database.</p> <p>The number of seconds between measurements by the exporters on the compute nodes:</p> <pre><code># number of seconds between measurements\nSAMPLING_PERIOD = 30\n</code></pre> <p>The value above should match that in the Prometheus configuration, i.e., <code>scrape_interval: 30s</code>.</p> <p>One can use the Python <code>blessed</code> package to produce bold and colorized text. This helps to draw attention to specific lines of the report. This part of the configuration sets the various thresholds:</p> <pre><code># threshold values for red versus black notes\nGPU_UTIL_RED   = 15  # percentage\nGPU_UTIL_BLACK = 25  # percentage\nCPU_UTIL_RED   = 65  # percentage\nCPU_UTIL_BLACK = 80  # percentage\nTIME_EFFICIENCY_RED   = 10  # percentage\nTIME_EFFICIENCY_BLACK = 60  # percentage\n</code></pre> <p>For instance, if the overal GPU utilization is less than 15% then it will be displayed in bold red text. Search the conditions in the example notes in <code>config.py</code> to see how the other values are used.</p> <p>The following optional settings can be added:</p> <pre><code># optional settings\nGPU_MEM_UTIL_RED = 25    # percentage\nGPU_MEM_UTIL_BLACK = 50  # percentage\nCPU_MEM_UTIL_RED = 50    # percentage\nCPU_MEM_UTIL_BLACK = 80  # percentage\n</code></pre> <p>The optional settings above are useful for institutions that calculate job priority based on the memory utilization of previous jobs.</p> <p>The following threshold can be used to trigger notes about excessive CPU memory usage:</p> <pre><code>MIN_MEMORY_USAGE = 70  # percentage\n</code></pre> <p>Notes can be suppressed if the run time of the job is less than the following threshold:</p> <pre><code>MIN_RUNTIME_SECONDS = 10 * SAMPLING_PERIOD  # seconds\n</code></pre> <p>Use <code>CLUSTER_TRANS</code> to convert informal cluster names to the name that is used in the Slurm database. For instance, if the <code>tiger</code> cluster is replaced by the <code>tiger2</code> cluster then use:</p> <pre><code>CLUSTER_TRANS = {\"tiger\":\"tiger2\"}\nCLUSTER_TRANS_INV = dict(zip(CLUSTER_TRANS.values(), CLUSTER_TRANS.keys()))\n</code></pre> <p>This will allow users to specify <code>tiger</code> as the cluster while internally the value <code>tiger2</code> is used when querying the Slurm database.</p> <p>One can trim long job names:</p> <pre><code># maximum number of characters to display in jobname\nMAX_JOBNAME_LEN = 64\n</code></pre>"},{"location":"setup/jobstats_command/#mig-gpu-nodes-optional","title":"MIG GPU Nodes (Optional)","text":"<p>At present, <code>jobstats</code> cannot report GPU utilization for NVIDIA MIG GPUs. To gracefully deal with this, specify the hostnames of your MIG GPU nodes:</p> <pre><code>MIG_NODES_1 = {\"della-l01g3\", \"della-l01g4\", \\\n               \"della-l01g5\", \"della-l01g6\"}\nMIG_NODES_2 = {\"adroit-h11g2\"}\n</code></pre> <p>There is no difference between <code>MIG_NODES_1</code> and <code>MIG_NODES_2</code>. The code combines them.</p> <p>If MIG is not used then leave empty:</p> <pre><code>MIG_NODES_1 = {}\nMIG_NODES_2 = {}\n</code></pre>"},{"location":"setup/jobstats_command/#custom-job-notes-optional","title":"Custom Job Notes (Optional)","text":"<p>Institutions that use the Jobstats platform have the ability to write custom notes in <code>config.py</code> that can appear at the bottom of the job efficiency reports. Here is a simple example that makes the user aware of the Grafana dashboard:</p> <pre><code>                                    Notes\n================================================================================\n  * See the URL below for various job metrics plotted as a function of time:\n      https://mytiger.princeton.edu/pun/sys/jobstats/12798795\n</code></pre> <p>Job notes can be used to provide information and to guide users toward solving underutilization issues such low GPU utilization or excessive CPU memory allocations.</p> <p>Each note is Python code that is composed of three items: (1) a <code>condition</code>, (2) the actual <code>note</code>, and (3) the <code>style</code>. The <code>condition</code> is a Python string that gets evaluated to <code>True</code> or <code>False</code> when <code>jobstats</code> is ran. The <code>note</code> is the text to be displayed. Lastly, the <code>style</code> sets the formatting which is either <code>normal</code>, <code>bold</code>, or <code>bold-red</code>.</p> <p>Consider the following note in <code>config.py</code>:</p> <pre><code>condition = '(self.js.cluster == \"tiger\") and self.js.is_retained()'\nnote = (\"See the URL below for various job metrics plotted as a function of time:\",\n        'f\"https://mytiger.princeton.edu/pun/sys/jobstats/{self.js.jobid}\"')\nstyle = \"normal\"\nNOTES.append((condition, note, style))\n</code></pre> <p>The note above will be displayed by <code>jobstats</code> for all jobs that ran on the <code>tiger</code> cluster.</p> <p>Much more sophisicated and useful notes can be constructed. For more ideas and examples, see the many notes that appear in <code>config.py</code> in the Jobstats GitHub repository.</p> <p>Notes can contain Slurm directives and URLs. These items are automatically displayed on a separate line with additional indentation.</p> <p>Warning</p> <p>System administrators should not give users the ability to add notes to <code>config.py</code> since in principle they could write malicious code that would be executed when <code>jobstats</code> is ran.</p> <p>If you decide not to use notes then keep <code>NOTES = []</code> in <code>config.py</code> but remove everything  below that line.</p> <p>Below are some example notes that are possible:</p> <pre><code>  * This job did not use the GPU. Please resolve this before running\n    additional jobs. Wasting resources causes your subsequent jobs to have a\n    lower priority. Is the code GPU-enabled? Please consult the documentation\n    for the code. For more info:\n      https://researchcomputing.princeton.edu/support/knowledge-base/gpu-computing\n\n  * This job used 6 GPUs from 6 compute nodes. The PLI GPU nodes on Della have\n    8 GPUs per node. Please allocate all of the GPUs within a node before\n    splitting your job across multiple nodes. For more info:\n      https://researchcomputing.princeton.edu/support/knowledge-base/slurm#gpus\n\n  * Each node on Della (PLI) has 96 CPU-cores and 8 GPUs. If possible please\n    try to allocate only up to 12 CPU-cores per GPU. This will prevent the\n    situation where there are free GPUs on a node but not enough CPU-cores to\n    accept new jobs. For more info:\n      https://researchcomputing.princeton.edu/systems/della\n\n  * Each node on Della (PLI) has 1024 GB of CPU memory and 8 GPUs. If possible\n    please try to allocate only up to 115 GB of CPU memory per GPU. This will\n    prevent the situation where there are free GPUs on a node but not enough\n    CPU memory to accept new jobs. For more info:\n      https://researchcomputing.princeton.edu/systems/della\n\n  * This job ran on the mig partition where each job is limited to 1 MIG\n    GPU, 1 CPU-core, 10 GB of GPU memory and 32 GB of CPU memory. A MIG GPU\n    is about 1/7th as powerful as an A100 GPU. Please continue using the mig\n    partition when possible. For more info:\n      https://researchcomputing.princeton.edu/systems/della\n\n  * This job should probably use a MIG GPU instead of a full A100 GPU. MIG is\n    ideal for jobs with a low GPU utilization that only require a single\n    CPU-core, less than 32 GB of CPU memory and less than 10 GB of GPU memory.\n    For future jobs, please add the following line to your Slurm script:\n      #SBATCH --partition=mig\n    For more info:\n      https://researchcomputing.princeton.edu/systems/della\n\n  * This job completed while only needing 19% of the requested time which\n    was 2-00:00:00. For future jobs, please decrease the value of the --time\n    Slurm directive. This will lower your queue times and allow the Slurm\n    job scheduler to work more effectively for all users. For more info:\n      https://researchcomputing.princeton.edu/support/knowledge-base/slurm\n\n  * This job only used 15% of the 100GB of total allocated CPU memory.\n    Please consider allocating less memory by using the Slurm directive\n    --mem-per-cpu=3G or --mem=18G. This will reduce your queue times and\n    make the resources available to other users. For more info:\n      https://researchcomputing.princeton.edu/support/knowledge-base/memory\n\n  * This job ran on a large-memory (datascience) node but it only used 117\n    GB of CPU memory. The large-memory nodes should only be used for jobs\n    that require more than 190 GB. Please allocate less memory by using the\n    Slurm directive --mem-per-cpu=9G or --mem=150G. For more info:\n      https://researchcomputing.princeton.edu/support/knowledge-base/memory\n\n  * The CPU utilization of this job (24%) is approximately equal to 1\n    divided by the number of allocated CPU-cores (1/4=25%). This suggests\n    that you may be running a code that can only use 1 CPU-core. If this is\n    true then allocating more than 1 CPU-core is wasteful. Please consult\n    the documentation for the software to see if it is parallelized. For\n    more info:\n      https://researchcomputing.princeton.edu/support/knowledge-base/parallel-code\n\n  * This job did not use the CPU. This suggests that something went wrong at\n    the very beginning of the job. Check your Slurm script for errors and\n    look for useful information in the file slurm-46987157.out if it exists.\n\n  * The Tiger cluster is intended for jobs that require multiple nodes. This\n    job ran in the serial partition where jobs are assigned the lowest\n    priority. On Tiger, a job will run in the serial partition if it only\n    requires 1 node. Consider carrying out this work elsewhere.\n\n  * See the URL below for various job metrics plotted as a function of time:\n      https://mytiger.princeton.edu/pun/sys/jobstats/12798795\n</code></pre> <p>Each institution that uses Jobstats is encouraged to write custom notes for their users.</p>"},{"location":"setup/node/","title":"Node Statistics","text":"<p>The Prometheus <code>node_exporter</code> should be setup to run on every compute node. This allows the Jobstats platform to obtain basic node metrics such as total memory available, memory usage, CPU frequencies, NFS statistics, Infiniband statistics and many other potentially useful quantities. Spectrum Scale/GPFS statistics are collected with a custom Python-based exporter.</p> <p>Proceed to the next section on Job Summaries.</p>"},{"location":"setup/ood/","title":"Open OnDemand Jobstats Helper","text":"<p>See the <code>ood-jobstats-helper</code> directory in the Jobstats GitHub repository. This directory contains an Open OnDemand app that, given a <code>jobid</code>, uses <code>sacct</code> to generate a full Grafana URL with the <code>jobid</code>, start time and end time.</p> <p>As of July 2025, the Open OnDemand helper app supports noninteractive redirect to the Grafana dashboard for a given <code>jobid</code>. This makes it possible to add notes such as the following to the <code>jobstats</code> output:</p> <pre><code>* See the URL below for various job metrics plotted as a function of time:\n    https://mydella.princeton.edu/pun/sys/jobstats/39798795\n</code></pre> <p>When a browser is pointed at the URL above, the Grafana webpage for the job (containing the 17 metrics versus time) is displayed.</p>"},{"location":"setup/overview/","title":"Setup Overview","text":"<p>Below is an outline of the setup of the Jobstats platform for a Slurm cluster:</p> <ol> <li>Switch to cgroup-based job accounting from Linux process accounting </li> <li>Setup the exporters: cgroup, node, GPU (on the nodes) and, optionally, GPFS (centrally)</li> <li>Setup the <code>prolog.d</code> and <code>epilog.d</code> scripts on the GPU nodes</li> <li>Setup the Prometheus server and configure it to scrape the data from the compute nodes</li> <li>Setup the <code>slurmctldepilog.sh</code> script for long-term job summary retention</li> <li>Lastly, configure the Grafana interface and Open OnDemand</li> </ol> <p>A single standard server has proven to be sufficient for a data center with 150,000 CPU-cores and 1000 GPUs.</p> <p>Proceed to the next section on cgroup-based job accounting.</p>"},{"location":"setup/prometheus/","title":"Prometheus","text":"<p>Prometheus is a monitoring system and time series database. For setup, follow the directions at prometheus.io. The Prometheus exporters required for the Jobstats platform were discussed in the previous sections.</p>"},{"location":"setup/prometheus/#basic-prometheus-configuration","title":"Basic Prometheus Configuration","text":"<p>Below is an example of the production configuration for the Tiger cluster that has both CPU and GPU nodes:</p> <pre><code>---\nglobal:\n  scrape_interval: 15s\n  evaluation_interval: 15s\n  external_labels:\n    monitor: master\n- job_name: Tiger Nodes\n  scrape_interval: 30s\n  scrape_timeout: 30s\n  file_sd_configs:\n  - files:\n    - \"/etc/prometheus/local_files_sd_config.d/tigernodes.json\"\n  metric_relabel_configs:\n  - source_labels:\n    - __name__\n    regex: \"^go_.*\"\n    action: drop\n- job_name: TigerGPU Nodes\n  scrape_interval: 30s\n  scrape_timeout: 30s\n  file_sd_configs:\n  - files:\n    - \"/etc/prometheus/local_files_sd_config.d/tigergpus.json\"\n  metric_relabel_configs:\n  - source_labels:\n    - __name__\n    regex: \"^go_.*\"\n    action: drop\n</code></pre> <p>The <code>tigernode.json</code> file looks like:</p> <pre><code> [\n   {\n     \"labels\": {\n       \"cluster\": \"tiger\"\n     },\n     \"targets\": [\n       \"tiger-h19c1n10:9100\",\n       \"tiger-h19c1n10:9306\",\n       ...\n     ]\n   }\n ]\n</code></pre> <p>Both <code>node_exporter</code> (port 9100) and <code>cgroup_exporter</code> (port 9306) are listed for all of the nodes in <code>tigernode.json</code>. The file <code>tigergpus.json</code> looks very similar except that it collects data from <code>nvidia_gpu_prometheus_exporter</code> on port 9445. Note the additional <code>cluster</code> label.</p>"},{"location":"setup/smail/","title":"Procedure for Modifying User Email Reports","text":"<p>The <code>jobstats</code> command should be configured to replace <code>smail</code>, which is the Slurm executable used for sending email reports. To make this change, edit <code>slurm.conf</code> as follows:</p> <pre><code>MailProg=/usr/local/bin/jobstats_mail.sh\n</code></pre> <p>The <code>jobstats_mail.sh</code> script is available in the <code>slurm</code> directory of the Jobstats GitHub repository. This script sets the <code>content-type</code> to <code>text/html</code> so that the email is sent using a fixed-width font.</p> <p>As always, to receive an email report, users must include the appropriate Slurm directive in their scripts:</p> <pre><code>#SBATCH --mail-type=end\n</code></pre> <p>Note</p> <p>When the run time of the job is less than the sampling period of the Prometheus exporters (which is typically 30 seconds), <code>jobstats</code> will call <code>seff</code> to generate the job report.</p>"},{"location":"setup/summaries/","title":"Generating Job Summaries","text":"<p>When all four exporters are used, the Prometheus database stores 17 metrics every N seconds for each job. It is recommended to retain this detailed data for several months or longer. This data can be visualized using the Grafana dashboard. After some amount of time, it makes sense to purge the detailed data while keeping only a summary (i.e., CPU/GPU utilization and memory usage per node). The summary data can also be used in place of the detailed data when generating efficiency reports.</p> <p>A summary of individual job statistics is generated at job completion and stored in the Slurm database (or MariaDB) in the <code>AdminComment</code> field. This is done by a <code>slurmctld</code> epilog script that runs at job completion. For example, in <code>slurm.conf</code>:</p> <pre><code>EpilogSlurmctld=/usr/local/sbin/slurmctldepilog.sh\n</code></pre> <p>The script is available in the Jobstats GitHub repository and needs to be installed on the <code>slurmctld</code> server along with Jobstats. For storage efficiency and convenience, the JSON job summary data is gzipped and base64 encoded before being stored in the <code>AdminComment</code> field of the Slurm database (or MariaDB).</p> <p>The impact on the database size due to this depends on job sizes. For an institution with 100,000 CPU-cores, for small jobs the <code>AdminComment</code> field tends to average under 50 characters per entry with a maximum under 1500 while for large jobs the maximum length is around 5000.</p> <p>For processing old jobs where the <code>slurmctld</code> epilog script did not run or for jobs where it failed, there is a per cluster ingest Jobstats service. This is a combination of a Python-based script <code>jobs_with_no_data.py</code> that returns a list of recent jobs with an empty AdminComment and a bash script <code>ingest_jobstats</code> that uses that utility to process those jobs and set AdminComment. Since the <code>jobs_with_no_data.py</code> needs db access it is easiest to run this on the <code>slurmdbd</code> host, either as a cron or a <code>systemd</code> timer and service. These scripts (<code>ingest_jobstats</code> and 'jobs_with_no_data.py') and <code>systemd</code> timer and service units are in the <code>slurm</code> directory of the Jobstats GitHub repository.</p> <p>Below is an example job summary for a GPU job:</p> <pre><code>$ jobstats 12345678 -j\n{\n    \"gpus\": 1,\n    \"nodes\": {\n        \"della-k1g1\": {\n            \"cpus\": 12,\n            \"gpu_total_memory\": {\n                \"1\": 85899345920\n            },\n            \"gpu_used_memory\": {\n                \"1\": 83314868224\n            },\n            \"gpu_utilization\": {\n                \"1\": 98.6\n            },\n            \"total_memory\": 137438953472,\n            \"total_time\": 57620.8,\n            \"used_memory\": 84683702272\n        }\n    },\n    \"total_time\": 50944\n}\n</code></pre> <p>The following pipeline shows how the summary statistics are recovered from an <code>AdminComment</code> entry in the database:</p> <pre><code>$ sacct -j 823722 -n -o admincomment%250 | sed 's/JS1://' | tr -d ' ' \\\n  | base64 -d \\\n  | gzip -d \\\n  | jq\n{\n  \"nodes\": {\n    \"della-l01g2\": {\n      \"total_memory\": 33554432000,\n      \"used_memory\": 27482066944,\n      \"total_time\": 8591.7,\n      \"cpus\": 1,\n      \"gpu_total_memory\": {\n        \"1\": 10200547328\n      },\n      \"gpu_used_memory\": {\n        \"1\": 124780544\n      }\n    }\n  },\n  \"total_time\": 432301,\n  \"gpus\": 1\n}\n</code></pre> <p>Only completed jobs have entries in the database.</p>"},{"location":"setup/summaries/#using-pandas-to-analyze-the-data","title":"Using <code>pandas</code> to Analyze the Data","text":"<p>The Python code below can be used to work with the summary statistics in <code>AdminComment</code>:</p> <pre><code>$ wget https://raw.githubusercontent.com/PrincetonUniversity/job_defense_shield/refs/heads/main/src/job_defense_shield/efficiency.py\n$ wget https://raw.githubusercontent.com/jdh4/saccta/refs/heads/main/gpu_usage.py\n</code></pre> <p>The script <code>gpu_usage.py</code> illustrates how to get the GPU utilization per job. The other functions in <code>efficiency.py</code> can be used to get the other metrics (e.g., <code>cpu_memory_usage</code>). See also the source code for Job Defense Shield where <code>efficiency.py</code> is used.</p>"},{"location":"setup/summaries/#analyzing-the-prometheus-data","title":"Analyzing the Prometheus Data","text":"<p>The summary statistics capture a small fraction of the total data associated with each job. To work with several of the metrics, one must query the Prometheus server.</p> <p>Here is an example of getting the mean power usage per GPU for a job that used 4 GPUs:</p> <pre><code>$ export SLURM_TIME_FORMAT=%s\n$ sacct -j 1191148 -X -o start,end\n              Start                 End \n------------------- ------------------- \n         1759675685          1759679604\n</code></pre> <p>The run time of the job is <code>1759679604 - 1759675685 = 3919</code> seconds.</p> <p>The Python script below can be used to obtain the mean power per GPU:</p> <pre><code>import json\nimport requests\n\nparams = {'query':'avg_over_time((nvidia_gpu_power_usage_milliwatts{cluster=\"della\"} and nvidia_gpu_jobId == 1191148)[3919s:])',\n          'time':1759679604}\nresponse = requests.get('http://vigilant2:8480/api/v1/query', params)\ndata = response.json()\n\nprint(json.dumps(data, indent=2))\n</code></pre> <p>The output is:</p> <pre><code>{\n  \"status\": \"success\",\n  \"data\": {\n    \"resultType\": \"vector\",\n    \"result\": [\n      {\n        \"metric\": {\n          \"cluster\": \"della\",\n          \"instance\": \"della-l02g8:9445\",\n          \"job\": \"Della GPU Nodes\",\n          \"jobid\": \"1191148\",\n          \"minor_number\": \"1\",\n          \"name\": \"NVIDIA A100 80GB PCIe\",\n          \"ordinal\": \"1\",\n          \"service\": \"compute\",\n          \"userid\": \"331233\",\n          \"uuid\": \"GPU-e8ba89df-0d52-4693-7098-1b38647e1462\"\n        },\n        \"value\": [\n          1759679595,\n          \"136074.7816091954\"\n        ]\n      },\n      {\n        \"metric\": {\n          \"cluster\": \"della\",\n          \"instance\": \"della-l02g8:9445\",\n          \"job\": \"Della GPU Nodes\",\n          \"jobid\": \"1191148\",\n          \"minor_number\": \"3\",\n          \"name\": \"NVIDIA A100 80GB PCIe\",\n          \"ordinal\": \"3\",\n          \"service\": \"compute\",\n          \"userid\": \"331233\",\n          \"uuid\": \"GPU-cd73f312-e09f-6884-cf10-00982b08d58a\"\n        },\n        \"value\": [\n          1759679595,\n          \"126442.79310344828\"\n        ]\n      },\n      {\n        \"metric\": {\n          \"cluster\": \"della\",\n          \"instance\": \"della-l02g8:9445\",\n          \"job\": \"Della GPU Nodes\",\n          \"jobid\": \"1191148\",\n          \"minor_number\": \"0\",\n          \"name\": \"NVIDIA A100 80GB PCIe\",\n          \"ordinal\": \"0\",\n          \"service\": \"compute\",\n          \"userid\": \"331233\",\n          \"uuid\": \"GPU-207d45af-87b6-798b-b87c-ad7c9e7f6c35\"\n        },\n        \"value\": [\n          1759679595,\n          \"130576.59770114943\"\n        ]\n      },\n      {\n        \"metric\": {\n          \"cluster\": \"della\",\n          \"instance\": \"della-l02g8:9445\",\n          \"job\": \"Della GPU Nodes\",\n          \"jobid\": \"1191148\",\n          \"minor_number\": \"2\",\n          \"name\": \"NVIDIA A100 80GB PCIe\",\n          \"ordinal\": \"2\",\n          \"service\": \"compute\",\n          \"userid\": \"331233\",\n          \"uuid\": \"GPU-aa72d60c-a2bd-d330-9ad2-61d0b2750c0b\"\n        },\n        \"value\": [\n          1759679595,\n          \"127405.17624521072\"\n        ]\n      }\n    ]\n  }\n}\n</code></pre> <p>There are four entries like <code>\"value\": [1759679595, \"127405.17624521072\"]</code> which give the power in milliWatts (e.g., 127405 mW or 127 W).</p>"},{"location":"tools/gpudash/","title":"GPU Dashboard","text":"<p>The <code>gpudash</code> command generates a text-based dashboard of the GPU utilization across a cluster in the form of a 2-dimensional grid. Each cell displays the utilization from 0-100% along with the username associated with each allocated GPU. Cells are colored according to their utilization values making it easy to identify jobs with low or high GPU utilization. The <code>gpudash</code> command can also be used to check for available GPUs.</p> <p>By default, the dashboard has seven columns and a number of rows equal to the number of GPUs on the cluster. Each column is evenly spaced in time by N minutes. We find a good choice is N=10 minutes which leads to data being shown over an hour. The <code>cron</code> utility can be used to achieve this. The rows are labeled by the node name and the GPU index while the columns are labeled by time.</p> <p>The <code>gpudash</code> command works by making the three queries to the Prometheus server every N minutes. A Python script is used to extract the information from the three generated JSON files and append this data to the files read by <code>gpudash</code>. The <code>UID</code> for each user is matched with its corresponding username. The <code>jobid</code> is not required but it can be useful for troubleshooting.</p> <p>Nodes that are down, or in a state which makes them unavailable, are not shown in the dashboard. Labels can be added to mark reserved nodes or special-purpose nodes.</p>"},{"location":"tools/gpudash/#installation","title":"Installation","text":"<p>The installation requirements for <code>gpudash</code> are Python 3.6+ and version 1.17+ of the Python <code>blessed</code> package which is used for creating colored text and backgrounds. The Python code and instructions are available at https://github.com/PrincetonUniversity/gpudash.</p>"},{"location":"tools/job_defense_shield/","title":"Job Defense Shield","text":"<p>Job Defense Shield is a software tool for identifying and reducing instances of underutilization by the users of high-performance computing systems. The software sends automated email alerts to users and creates reports for system administrators. Job Defense Shield can be used to automatically cancel GPU jobs at 0% utilization.</p> <p>Below is an example report for 0% GPU utilization:</p> <pre><code>                         GPU-Hours at 0% Utilization\n---------------------------------------------------------------------\n    User   GPU-Hours-At-0%  Jobs             JobID             Emails\n---------------------------------------------------------------------\n1  u12998        308         39   62285369,62303767,62317153+   1 (7)\n2  u9l487         84         14   62301737,62301738,62301742+   0         \n3  u39635         25          2            62184669,62187323    2 (4)         \n4  u24074         24         13   62303182,62303183,62303184+   0         \n---------------------------------------------------------------------\n   Cluster: della\nPartitions: gpu, llm\n     Start: Wed May 14, 2025 at 09:50 AM\n       End: Wed May 21, 2025 at 09:50 AM\n</code></pre> <p>Below is an example email to a user that is requesting too much CPU memory:</p> <pre><code>Hi Alan (u12345),\n\nBelow are your jobs that ran on the Stellar cluster in the past 7 days:\n\n     JobID   Memory-Used  Memory-Allocated  Percent-Used  Cores  Hours\n    5761066      2 GB          100 GB            2%         1     48\n    5761091      4 GB          100 GB            4%         1     48\n    5761092      3 GB          100 GB            3%         1     48\n\nIt appears that you are requesting too much CPU memory for your jobs since\nyou are only using on average 3% of the allocated memory. For help on\nallocating CPU memory with Slurm, please see:\n\n    https://your-institution.edu/knowledge-base/memory\n\nReplying to this automated email will open a support ticket with Research\nComputing.\n</code></pre>"},{"location":"tools/overview/","title":"Overview of Jobstats Tools","text":"<p>The Jobstats platform provides a rich foundation for building powerful tools. In this section we describe four such tools:</p> <ul> <li>Job Defense Shield</li> <li>GPU Dashboard</li> <li>Utilization Reports</li> <li>reportseff</li> </ul>"},{"location":"tools/reportseff/","title":"<code>reportseff</code>","text":"<p>The <code>reportseff</code> utility wraps <code>sacct</code> to provide a cleaner user experience when interrogating Slurm job efficiency values for multiple jobs. In addition to multiple jobids, <code>reportseff</code> accepts Slurm output files as arguments and parses the jobid from the filename. Some <code>sacct</code> options are further wrapped or extended to simplify common operations. The output is a table with entries colored based on high/low utilization values. The columns and formatting of the table can be customized based on command line options.</p> <p>A limit to the previous tools is that they provide information on a single job at a time in great detail. Another common use case is to summarize job efficiency for multiple jobs to gain a better idea of the overall utilization. Summarized reporting is especially useful with array jobs and workflow managers which interface with Slurm. In these cases, running <code>seff</code> or <code>jobstats</code> becomes burdensome.</p>"},{"location":"tools/reportseff/#usage","title":"Usage","text":"<p>The <code>reportseff</code> tool accepts jobs as jobids, Slurm output files, and directories containing Slurm output files:</p> <pre><code>$ reportseff 123 124      # get information on jobs 123 and 124\n$ reportseff {123..133}   # get information on jobs 123 to 133\n$ reportseff jobname*     # check output files starting with jobname\n$ reportseff slurm_out/   # look for output files in the slurm_out directory\n</code></pre> <p>The ability to link Slurm outputs with job status simplifies locating problematic jobs and cleaning up their outputs. The <code>reportseff</code> utility extends some of the <code>sacct</code> options. The start and end time can accept any format accepted by <code>sacct</code>, as well as a custom format, specified as a comma separated list of key/value pairs. For example:</p> <pre><code>$ reportseff --since now-27hours   # equivalent to\n$ reportseff --since d=1,h=3       # 1 day, 3 hours\n</code></pre> <p>Filtering by job state is expanded with <code>reportseff</code> to specify states to exclude. This filtering combined with accepting output files helps in cleaning up failed output jobs:</p> <pre><code>$ reportseff --not-state CD     # not completed\n             --since d=1 \\      # today\n             --format=jobid \\   # just get file name\n             my_failing_job* \\  # only from these outputs\n             | xargs grep \"output:\"\n</code></pre> <p>The last piece of the pipeline above find lines with the output directive to examine or delete. The format option can accept a comma-separated list of column names or additional columns can be appended to the default values. Appending prevents the need to add in the same, default columns on every invocation.</p> <p>While the above features are available for any Slurm system, when Jobstats information is present in the <code>AdminComment</code>, the multi-node resource utilization is updated with the more accurate Jobstats values and GPU utilization is also provided. This additional information is controlled with the <code>--node</code> and <code>--node-and-gpu</code> options.</p> <p>A sample workflow with <code>reportseff</code> is to run a series of jobs, each producing an output file. Run <code>reportseff</code> on the output directory to determine the utilization and state of each job. Jobs with low utilization or failure can be examined more closely by copy/pasting the Slurm output filename from the first column. Outputs from failed jobs can be cleaned automatically with a version of the command piping above. Combining with watch and aliases can create powerful monitoring for users:</p> <pre><code># monitor the current directory every 5 minutes\n$ watch -cn 300 reportseff --modified-sort\n\n# monitor the user's efficiency every 10 minutes\n$ watch -cn 600 reportseff --user $USER --modified-sort --format=+jobname\n</code></pre>"},{"location":"tools/reportseff/#installation","title":"Installation","text":"<p>The installation requirements for reportseff are Python 3.7+ and version 6.7+ of the Python <code>click</code> package which is used for creating colored text and command-line parsing. The Python code and instructions are available at https://github.com/troycomi/reportseff.</p>"},{"location":"tools/utilization_reports/","title":"Utilization Reports","text":"<p>This is a tool for sending detailed usage reports to users or group leaders by email.</p> <p>Users can receive an email utilization report upon completion of each job via Slurm directives. Because some users decide not to receive these emails, it is important to periodically send a comprehensive utilization report to each user. As discussed earlier, summary statistics for each completed job are stored in a compressed format in the <code>AdminComment</code> field of the Slurm database. The software described here works by calling <code>sacct</code> while requesting several fields including <code>AdminComment</code>. The <code>sacct</code> output is stored in a <code>pandas</code> dataframe for processing.</p> <p>Each user that ran at least one Slurm job in the specified time interval will receive a report when the software is run. The first part of the report is a table that indicates the overall usage for each cluster. Each row provides the CPU-hours, GPU-hours, number of jobs, and Slurm account(s) and partition(s) that were used by the user.</p> <p>The second part of the report is a detailed table showing for each partition of each cluster the CPU-hours, CPU-rank, CPU-eff, GPU-hours, GPU-rank, GPU-eff and number of jobs. The CPU-rank or GPU-rank indicates the user's usage relative to the other users on the given partition of the cluster. CPU-eff (or GPU-eff) is the overall CPU (or GPU) efficiency which varies from 0-100%. A responsible user will take action when seeing that their rank is high while their efficiency is low. The email report also provides a definition for each reported quantity. The software could be extended by adding queue hours and data storage information to the tables.</p>"},{"location":"tools/utilization_reports/#usage","title":"Usage","text":"<p>The default mode of the software is to send user reports. It can also be used to send reports to those that are responsible for the users such as the principal investigator. This is the so-called <code>sponsors</code> mode. The example below shows how the script is called to generate user reports over the past month which are sent by email:</p> <pre><code>$ utilization_reports --report-type=users --months=1 --email\n</code></pre>"},{"location":"tools/utilization_reports/#installation","title":"Installation","text":"<p>We find a good choice is to send the report once per month. The installation requirements for the software are Python 3.6+ and version 1.2+ of the <code>pandas</code> package. The Python code, example reports, and instructions are available at https://github.com/PrincetonUniversity/monthly_sponsor_reports.</p>"}]}